---
title: 'ClubReady Digital Insights Workshop'
output:
    rmdformats::readthedown:
      highlight: pygments
      code_folding: hide
---

<style type="text/css">
p{ /* Normal  */
   font-size: 14px;
   line-height: 18px;
}
body{ /* Normal  */
   font-size: 14px;
}
td {  /* Table  */
   font-size: 12px;
}
h1 { /* Header 1 */
font-size: 26px;
color: #4294ce;
}
h2 { /* Header 2 */
font-size: 22px;
}
h3 { /* Header 3 */
font-size: 18px;
}
code.r{ /* Code block */
  font-size: 12px;
}
pre { /* Code block */
  font-size: 12px
}
#table-of-contents h2 {
background-color: #4294ce;
}
#table-of-contents{
background: #688FAD;
}
#nav-top span.glyphicon{
color: #4294ce;
}
#postamble{
background: #4294ce;
border-top: ;
}
</style>

```{r loadLibs1, warning=FALSE, message=FALSE}
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("plyr", "dplyr", "data.table", "xda","ggplot2", "forcats", "readr", "gridExtra", "knitr", "onehot", 
        "tidyr", "purrr", "survival",  prompt = FALSE)
```

# Abstract

ClubReady asked Valorem to assist in the development on machine learning algorithms to predict user churn and store churn.  After an thorough exploratory data analysis, the data provided sufficient information to develop an algorithm that appears to predict store churn with relatively high accuracy.  However, the data does not provide sufficient information to predict user churn.  

# Introduction

This document details the processes and outcomes from a Valorem Digital Insight Workshop to predict churn models. 

## Client Description

ClubReady LLC operates a Web-based club management platform for fitness facilities, fitness individuals and large corporate chains. It specializes in member management, billing, EFT and POS and sales process/CRM. The company was incorporated in 2014 and is based in St. Louis, Missouri.

## Client Business Problem

ClubReady is the 3rd largest company in their industry.  The company recognizes the need to leverage their data assets to gain deeper knowledge and understanding of customer behaviors.  ClubReady has taken steps in this area to begin analyzing the significant data it captures through its membership software applications, and seeks support to further explore its data assets.  ClubReady reached out to Valorem as a preferred Microsoft partner with expertise in Advanced Analytics, including Azure Machine Learning and the Cortana Intelligence Suite of Azure services.   

ClubReady strives to become *a data-first* company focused on membership and member retention insights gained through an exploration of ClubReady data.

## Client Engagement

On October 3, 2017, ClubReady entered into an agreement with Valorem statement for a Digital Insight Workshop.  The workshop provided the following deliverables

- Data Assessment and Quality Report 
- Digital Analytics Vision & Roadmap of Actionable Insights 
- Documented Key Priorities 
- Potential Digital Insights Program ROI 
- Recommended and Prioritized Analytics Actions 
- Analytics Roadmap and Estimates 
- Presentation of Next Steps

> As documented later in the report, these deliverables were collapsed into fewer deliverables as agreed during the on-site workshop meeting.

## Digital Insights Process

Per the SOW, the Digital Insights Workshop Project flow includes:

1. Pre-Work and Agenda
2. On-Site Workshop
3. Analytics Run
4. Present Findings

> Analytics Run is renamed in this document to Exploratory Data Analysis

## Pre-Work and Agenda

On November 20, 2017, Valorem's Project Manager led an introductory pre-workshop Skype Meeting.  Andy Sweet, the ClubReady CTO and Project Sponsor participated with Valorem Data Scientists.  The meeting consisted of:

- Team Introductions
- Project Overview
- Communications Plan
- Q & A

The core project Team Members were identified:

| Team Member | Company & Title |
| --------------| ----------------------------------- |
| Andy Sweet | ClubReady CTO |
| Justin Trusty | ClubReady Data Architect |
| Lauren Crosby | ClubReady Director of Product Management |
| Matt Mercurio | ClubReady Director of Engineering |
| Brad Llewellyn | Valorem Data Scientist |
| Cliff Weaver | Valorem Data Scientist |
| Brian Roselli | Valorem Project Management |

## On-Site Workshop

The on-site workshop was held at ClubReady on November 28 - 29.  The agenda presented at the workshop included:

- Day 1
    - ClubReady vision, goals, priorities
    - What can machine learning do for you?
    - Problem statements 
    - Redefine deliverables
    - Churn Example
    - Data Sources & Definitions
- Day 2
    - Review previous day successes and misses
    - Data Exploration
    - Q&A
    - Time Permitting (Clients Choice)
    - Introduction to R

During the two-day workshop, the agenda was roughly followed.  Regardless of the path, all the objectives to the workshop were accomplished:

- Understanding ClubReady goals and priorities
- Development of well-formed questions
- Review and Modification to Deliverables and Format
- Access to data
- Selection of data for analysis

### Understanding ClubReady Goals & Priorities

At the start of the on-site workshop, Andy Sweet provided an overview of ClubReady, its FY18 Goals and the mission to become a *data-first* company believing this will provide a strategic advantage over its competitors.  

### Well-Formed Question Development

The success of any data science project starts with a well-formed question.  A well-formed question is a prerequisite to a project because without it, the project is  likely to fail.  At a minimum, a well-formed question provides:

- A statement of the problem or issue that needs to be solved.
- Detailed description of the data available for analysis.  This includes data that is not not available.
- A description of what ClubReady would like to be able to predict or categorize. 
- How ClubReady will consume the output from this project (assuming the data supports an algorithmic solution).

Two questions were identified:

1. Identify what clubs (full service and DIY) are likely to fail based on percentage drop in revenue.  It was determined an 80% drop in revenue compared to the previous month(s) is the comparison metric.  Note:  Clubs were analysed and not separated into full service and DIY populations.  
2. Identify the individual customer propensity at the club level (lowest organization in the hierarchy) to terminate their club agreement.  (This excludes one-time users - i.e., not club members.)  Member Churn is defined by an individual customer agreement termination with no renewal within 30 days.

### Review and Modification to Deliverables and Format

The Statement of Work provided the following outputs:

- Data Assessment and Quality Report
- Digital Analytics Vision & Roadmap of Actionable Insights
- Documented Key Priorities 
- Potential Digital Insights Program ROI
- Recommended and Prioritized Analytics Actions
- Analytics Roadmap and Estimates
- Presentation of Next Steps

During the workshop, example documentation built using RMarkdown language saved in HTML format was presented.  The ClubReady Team agreed this format was acceptable for all project reporting.  (This document is the result of that agreement.)

### Access to ClubReady Data

On the 2nd day of the workshop, Valorem was granted and verified access to ClubReady data.

### Data Selection

ClubReady reviewed the tables and variables available in the ClubReady database (over 600 tables and 3,000+ fields).  It was quickly realized a majority of the variables were sparsely populated.  The Team implemented custom SQL code to identify the sparsity of each variable.  The Team then identified the initial set of candidate data ClubReady believed to be important to answer the well-formed questions.  Recognize the data available for analysis was a small subset of the all the database data. 

### Data Caveats & Assumptions

- There was no method to identify when data was populated.  If a column was 60% dense, the field would not have been selected even if that variable is 100% dense in the last year or two.  There may be useful data that was not used in this analysis.
- There is no guarantee all impactful variables were identified.  Reviewing a large dataset with a small Team does not ensure all important variables were included in the data used for modeling.
- Team decided the last 2 years of data would be used for the project.  It was in this period ClubReady experienced significant growth.

# Exploratory Data Analysis

This section of the document presents the results of the Exploratory Data Analysis process.  `stores.csv` data file is explored first in detail followed by `ClassesService.csv` and `users.csv` with less commentary content.

This document section is organized as follows:

- One of three CSV data files, `stores.csv`, is explored with detail in the first section
- In the following sections, the other two data files `(User` and `Class & Services`) are explored albeit with less explanatory content
- All of these sections roughly follow the same thought process:
    - Get Data
    - Remove/transform NAs
    - Character to Factors
    - Explore factors with tables and plots
    - Explore numerical data
    - Explore variances
    - Manage duplicate records
    - Data Glimpse

## Stores

### Get Data

Working with ClubReady, Valorem developed SQL scripts capturing raw data ClubReady data in csv format.  The SQL code will be provided upon request.

```{r loadData, echo=FALSE, message=FALSE,warning=FALSE, results='hide'}
stores <- read_csv("C:/Users/cweaver/Downloads/Stores_2017_12_11.csv", 
    col_types = cols(showIDphoto = col_integer(), 
        showaboutus = col_integer(), showactivity = col_integer(), 
        showarticles = col_integer(), showbalances = col_integer(), 
        showcusttodo = col_integer(), showdiscussion = col_integer(), 
        showfastfacts = col_integer(), showfitevals = col_integer(), 
        showgoals = col_integer(), showjournal = col_integer(), 
        shownews = col_integer(), shownutrition = col_integer(), 
        showphotos = col_integer(), showprogreport = col_integer(), 
        showprovtodo = col_integer(), showpurchhistory = col_integer(), 
        showscheduling = col_integer(), showstaffbios = col_integer()))
```

The `stores` data returns `r dim(stores[1])` records with `r dim(stores[2])` variables.  Of the `r dim(stores)[2]` variables, `r nrow(stores %>% select_if(is.character))` is a `character` type and `r nrow(stores %>% select_if(is.numeric))` are `numeric`.

### Categorical Variables

Examine the categorical variables first:

```{r}
charSummary(stores)
```

We learn the only categorical variable is `Status`.  `Status` is a factor with several levels. 

```{r}
stores <- stores %>% mutate_if(is.character, as.factor)
#charSummary(stores)
table(stores$Status)
ggplot(stores, aes(fct_infreq(Status))) + geom_bar() + xlab("ClubReady Status Code") + theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

ClubReady provided direction on how to manage `Active`, `Inactive`, and `Cancel`:

*Inactive is a store that is no longer with ClubReady.  Most likely left to go to competitor.*
*Cancel is a store that was in the process of being setup and for some reason stopped.*
*Ignore cancel.*

Remove records where `Status` is not either `Active` or `Inactive`.

```{r}
stores <-  filter(stores, Status == 'Active' | Status == 'Inactive')
table(stores$Status)
stores$Status <- factor(stores$Status)
table(stores$Status)
```

After removing the records associated with `Status` fields, `r nrow(stores)` records remain.

### Numerical Data

Review the numerical data.  Pay attention to the missing data percentage in shown in the right-most column below.  (Note, only showing the first 20 of the `r nrow(stores %>% select_if(is.numeric))` numerical variables.

```{r storeVarTypeNames}
myNumSum <- numSummary(stores)[, c(1,7,8,16,17)]
myNumSum <- tibble::rownames_to_column(myNumSum)
names(myNumSum)[5] <- "missCNT"
names(myNumSum)[1] <- "Variable_Name"
myNumSum <- arrange(myNumSum, desc(n))
head(myNumSum, 20)
```

Good, no missing data!

Examine the numerical data visually to illustrate interesting distributions.

```{r plotsNumeric, message=FALSE, warning=FALSE}
cntNumNames <- length(select(select_if(stores,is.numeric), -StoreId))
#Make plots max 6 at a time - change if needed
maxPlot = 6
loopCnt <- cntNumNames %/% maxPlot
remainder <- cntNumNames %% maxPlot

myLoop_DF <- data.frame(x = seq(1, cntNumNames-remainder, by = maxPlot), y = seq(6, cntNumNames, by = maxPlot))
myLoopMax <- max(myLoop_DF)

for(i in 1:nrow(myLoop_DF)){
  myplot <- select(select_if(stores,is.numeric), -StoreId)[myLoop_DF[i,1]:myLoop_DF[i,2]]%>% gather() %>% ggplot(aes(value)) +
      facet_wrap(~ key, scales = "free") + geom_histogram() #+  geom_density()
  print(myplot)
}
```

Most of the plots above are not very interesting but there are a few that we might want to revisit individually including `Amenities`, `Forms`, and `NonRequiredForms`.

```{r}
p1 <- ggplot(filter(stores, Amenities > 0), aes(Amenities)) + geom_bar() + ggtitle("Amenties > 0")
p2 <- ggplot(filter(stores, Forms > 0), aes(Amenities)) + geom_bar() + ggtitle("Forms > 0")
p3 <- ggplot(filter(stores, NonRequiredForms > 0), aes(Amenities)) + geom_bar() + ggtitle("NonRequiredForms > 0")
grid.arrange(p1, p2, p3, ncol=3)
```

#### Variability

Evaluate how much variability there is in each numerical variable.

```{r message=FALSE, warning=FALSE}
#Col 1 -s StoreId, 20 is Status
myVariance <- as.data.frame(apply(stores[,-c(1,20)], 2, var))
myVariance <- tibble::rownames_to_column(myVariance)
names(myVariance)[2] <- "Variance"
myVariance <-  myVariance %>% mutate(Variance2 = ifelse(Variance == 0, "No", "Yes"))
table(myVariance$Variance2)
```

Because `r table(myVariance$Variance2)[1]` variables have no variance - all the values are the same, they can be removed from the working dataset.  If there are no differences in a column, it is of no use in the development of an algorithm.  The variables removed because there is no variance are:

```{r}
VarNames <- myVariance %>% filter(Variance > 0) %>% select(rowname)
zeroVarNames <- myVariance %>% filter(Variance == 0) %>% select(rowname)
stores <- stores %>% select(StoreId, Status, unlist(VarNames))
zeroVarNames
```

#### Outlier Detection

In the working dataset, there is one variable, `TotalRevenue` to be evaluated for potential outliers.  There are many way to visualize outliers.  Boxplots are the most commonly used visualization.

```{r}
out2 <- ggplot(stores, aes(x = "", y = TotalRevenue)) + geom_boxplot(outlier.color="red", outlier.shape=8, outlier.size=4) + 
  scale_y_continuous(labels = scales::dollar)
```

There are potential outliers.  ClubReady may want to review the legitmacy of these values.  

Here is a list of the highest 25 TotalRevenue records:

```{r}
tmpRev <- arrange(stores, desc(TotalRevenue)) %>% select(TotalRevenue)
tmpRev <- as.data.frame(head(scales::dollar(tmpRev$TotalRevenue), 25))
names(tmpRev) <- "Total_Revenue"
tmpRev
```

#### Duplicates

Lastly, check for duplicate records.  In this case, none are found because each record has a unique `StoreId` value.  Without this variable, you would find `r nrow(stores[,-1]) - nrow(unique(stores[,-1]))` duplicates. (Interesting in its own right.)

```{r dupes}
# Duplicate Records
cat("The number of duplicated rows is", nrow(stores) - nrow(unique(stores)))

# if(nrow(stores) - nrow(unique(stores)) > 0){
#   head(stores[duplicated(stores),])
#   stores <- stores[!duplicated(stores),]
# }
```

#### Data Overview

The working dataset has been initially scrubbed and evaluated.  Take time to review and learn about the data.

The plots below illustrate how often many of the ClubReady configuration options are turned on.

First examine the variable names that start with `Integration`. 

```{r}
myColTotal <- as.data.frame(colSums(Filter(is.numeric, stores)))
myColTotal <- tibble::rownames_to_column(myColTotal)
names(myColTotal)[1] <- "Variable_Name"
names(myColTotal)[2] <- "Sum_of_Variable"
myColTotal <- filter(myColTotal, !Variable_Name %in% c("StoreId", "TotalRevenue", "Amenities"))
myColTotal$Variable_Name <- as.factor(myColTotal$Variable_Name)
myColTotal <- myColTotal %>% arrange(desc(Sum_of_Variable))

myColTotal_Int <- filter(myColTotal, Variable_Name %like% "Integration")
ggplot(myColTotal_Int, aes(x=Variable_Name, y=Sum_of_Variable)) + 
  geom_bar(stat = "identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  xlab("Variable Name") + ylab("Number of Times Option Selected") + scale_x_discrete(limits= myColTotal_Int$Variable_Name)
```

Six options appear to be much more popular than the others:

1. Listen360
2. Surveys
3. Club Management System
4. Rewards Program
5. Perkville
6. Data Trak

Below we find that most checkins occur between 8-11AM and 4-7PM local time.  (Need to confirm)

```{r}
myColTotal_checkins <- filter(myColTotal, Variable_Name %like% "Checkins_H" )
myColTotal_checkins <- myColTotal_checkins[!grepl("Months", myColTotal_checkins$Variable_Name),]
myColTotal_checkins$Variable_Name <- plyr::revalue(myColTotal_checkins$Variable_Name, c("Checkins_Hour0to1" = "0100", 
            "Checkins_Hour1to2" = "0200", 
            "Checkins_Hour2to3" = "0300", "Checkins_Hour3to4" = "0400", "Checkins_Hour4to5" = "0500",
            "Checkins_Hour5to6" = "0600", "Checkins_Hour6to7" = "0700", "Checkins_Hour7to8" = "0800",
            "Checkins_Hour8to9" = "0900", "Checkins_Hour9to10" = "1000", "Checkins_Hour10to11" = "1100",
            "Checkins_Hour11to12" = "1200", "Checkins_Hour12to13" = "1300", "Checkins_Hour13to14" = "1400",
            "Checkins_Hour14to15" = "1500", "Checkins_Hour15to16" = "1600", "Checkins_Hour16to17" = "1700", 
            "Checkins_Hour17to18" = "1800", "Checkins_Hour18to19" = "1900", "Checkins_Hour19to20" = "2000",
            "Checkins_Hour20to21" = "2100", "Checkins_Hour21to22" = "2200", "Checkins_Hour22to23" = "2300",
            "Checkins_Hour23to0" = "2400"))

myColTotal_checkins$Variable_Name <- factor(myColTotal_checkins$Variable_Name, 
      levels = c("0100", "0200", "0300", "0400", "0500", "0600", "0700", "0800", "0900", "1000", "1100", "1200",
                "1300", "1400", "1500", "1600", "1700", "1800", "1900", "2000", "2100", "2200", "2300", "2400"))

ggplot(myColTotal_checkins, aes(x=Variable_Name, y=Sum_of_Variable)) + 
  geom_bar(stat = "identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  xlab("Variable Name") + ylab("Number of Times Option Selected")
```

Lastly, Monday, Tuesday and Wednesday have the most checkins during the week.

```{r}
myColTotal_dow <- myColTotal %>% filter(!Variable_Name %like% "Checkins_H", Variable_Name %like% "Checkins_")
myColTotal_dow$Variable_Name <- substring(myColTotal_dow$Variable_Name, 10, 10000)
myColTotal_dow$Variable_Name <- as.factor(myColTotal_dow$Variable_Name)

myColTotal_dow2 <- myColTotal_dow[!grepl("_", myColTotal_dow$Variable_Name),]
myColTotal_dow2_1 <- myColTotal_dow2[grepl("day", myColTotal_dow2$Variable_Name),]
myColTotal_dow2_1 <- myColTotal_dow2_1[!grepl("Weekday", myColTotal_dow2_1$Variable_Name),]
myColTotal_dow2_1$Variable_Name <- factor(myColTotal_dow2_1$Variable_Name, levels = c("Monday", "Tuesday", "Wednesday",
                                  "Thursday", "Friday", "Saturday", "Sunday"))

ggplot(myColTotal_dow2_1, aes(x=Variable_Name, y=Sum_of_Variable)) + 
  geom_bar(stat = "identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  xlab("Variable Name") + ylab("Number of Times Option Selected")
```

### Data Glimpse

Here is what the resulting working dataset looks like.  We are left with `r nrow(stores)` records and `r ncol(stores)` (we began the journey with 5321 records and 88 variables).

```{r datatablestores}
glimpse(stores)
```

## Classes & Services

```{r clearObjectsApp1, echo=FALSE}
rm(list= ls())
```

### Get Data

```{r loadClassesServices, message=FALSE, warning=FALSE}
#Get Data
Classes_Services <- read_csv("C:/Users/cweaver/Downloads/Classes_Services.csv", 
                   col_types = cols(AvailablePIF = col_integer(), 
                      ByPerson = col_integer(), CanSeeInstructor = col_integer(), 
                      CancellationHrs = col_integer(), 
                      ClassMins = col_integer(), CustSelfBook = col_integer(), 
                      Disabled = col_integer(), EmailReminders = col_integer(), 
                      HalfHour = col_integer(), MultipleInstructors = col_integer(), 
                      MustHaveCredit = col_integer(), MustPreBook = col_integer(), 
                      NumPerClass = col_integer(), OnTheHour = col_integer(), 
                      QuarterPast = col_integer(), QuarterTill = col_integer(), 
                      RescheduleDeadline = col_integer(), 
                      RuleCustomers = col_integer(), RuleFrontDesk = col_integer(), 
                      SMSReminders = col_integer(), ServicesId = col_integer(), 
                      ShowPublic = col_integer(), StandardPrice = col_integer()), 
                   na = "NA")

myData <- Classes_Services
rm(Classes_Services)
```

`ClassesServices.csv` returned `r dim(myData)[1]` records with `r dim(myData)[2]` variables.  Of the `r dim(myData)[2]` variables, `r ncol(myData %>% select_if(is.character))` is a `character` type and `r ncol(myData %>% select_if(is.numeric))` are `numeric`.

```{r}
glimpse(myData)
```

### Remove NA

Just as before, remove the `NAs` with 0s:

```{r}
myData <- myData %>%  mutate_if(is.integer, funs(replace(., is.na(.), 0)))
myData <- myData %>%  mutate_if(is.double, as.integer)
glimpse(myData)
```

That looks much better!

### Character to Factors

Change the variable `Type` from a character to a factor.

```{r}
myData <- myData %>% mutate_if(is.character, as.factor)
class(myData$Type)
```

### Explore Factors

```{r}
charSummary(myData)
```

```{r}
myData_factor <- myData %>% select_if(is.factor)

for(i in 1:length(myData_factor)){
  print(names(myData_factor[i]))
  print(table(myData_factor[i]))
}

for(i in 1:length(myData_factor)){
  print(ggplot(myData_factor, aes_string(names(myData_factor[i]))) + geom_bar())
}
```

### Numerical Data

```{r}
myNumSum <- numSummary(myData)[, c(1,7,8,16,17)]
myNumSum <- tibble::rownames_to_column(myNumSum)
names(myNumSum)[5] <- "missPCT"
names(myNumSum)[1] <- "Variable_Name"
myNumSum <- arrange(myNumSum, desc(missPCT))
head(myNumSum, 20)
```

Good, no missing data!

#### Variances

```{r}
myVariance <- as.data.frame(apply(myData[,-c(1)], 2, var))
myVariance <- tibble::rownames_to_column(myVariance)
names(myVariance)[2] <- "Variance"
myVariance <-  myVariance %>% mutate(Variance2 = ifelse(Variance == 0, "No", "Yes"))
table(myVariance$Variance2)
```

All the variables have a variance > 0.

```{r}
if(table(myVariance$Variance2)[1] > 0){
  filter(myVariance, Variance2 == "No")
  VarNames <- myVariance %>% filter(Variance > 0) %>% select(rowname)
  myData <- myData %>% select(StoreId, unlist(VarNames))
}
```

#### Duplicate Records

If duplicates are found, they will be removed.

```{r}
cat("The number of duplicated rows is", nrow(myData) - nrow(unique(myData)))
```

```{r showDupes}
myData[duplicated(myData),]
```

```{r eval=FALSE}
if(nrow(myData) - nrow(unique(myData)) > 0){
  head(myData[duplicated(myData),])
  myData <- myData[!duplicated(myData),]
}
```

### Data Glimpse

```{r}
glimpse(myData)
```

## Users

```{r clearObjectsApp2, echo=FALSE}
rm(list= ls())
```

### Get Data

```{r message=FALSE, warning=FALSE}
Users <- read_csv("C:/Users/cweaver/Downloads/Users.csv", col_types = cols(StoreId = col_integer()), progress = FALSE)
myData <- Users
rm(Users)
```

`Users.csv` returned `r dim(myData)[1]` records with `r dim(myData)[2]` variables.  Of the `r dim(myData)[2]` variables, `r ncol(myData %>% select_if(is.character))` is a `character` type and `r ncol(myData %>% select_if(is.numeric))` are `numeric`.

```{r}
glimpse(myData)
```

### Remove NA

There do not appear to be as many `NAs` as we have seen before.  This time they appear prevalent in the `StoreId` variable.  Also note `NULL` in the `Gender` field.

```{r message=FALSE, warning=FALSE}
myData <- myData %>%  mutate_if(is.integer, funs(replace(., is.na(.), 0)))#changes int to dbl
myData[,-41] <- myData %>%  mutate_if(is.double, as.integer)#return to int
glimpse(myData$StoreId)
```

`StoreId` looks better now.

### Character to Factors

`Gender` and `UserType` are character variables.  Change them to factors.

```{r}
myData <- myData %>% mutate_if(is.character, as.factor)
charSummary(myData)
myData_factor <- myData %>% select_if(is.factor)
```

Note `Gender` has many missing values.  These will be managed later.

#### Tables for the Factors

Examine the factors in the working dataset.

```{r}
for(i in 1:length(myData_factor)){
  print(names(myData_factor[i]))
  print(table(myData_factor[i]))
}
rm(myData_factor)
```

There is much work to do on the `Gender` variable.  Will also choose the appropriate `UserType` values for modeling.

#### User Factor - Gender

The values in `Gender` are varied.  These will need to be collapsed into a couple of factor levels.

```{r}
myData %>% group_by(Gender) %>% summarize(Unique_Values = n()) %>% arrange(desc(Unique_Values))
ggplot(myData, aes(fct_infreq(Gender))) + geom_bar() + xlab(paste("ClubReady Subset - ", names(myData)[1])) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
levels(myData$Gender)[levels(myData$Gender) == "F"] <- "Female"
levels(myData$Gender)[levels(myData$Gender) == "female"] <- "Female"
levels(myData$Gender)[levels(myData$Gender) == "f"] <- "Female"

levels(myData$Gender)[levels(myData$Gender) == "M"] <- "Male"
levels(myData$Gender)[levels(myData$Gender) == "male"] <- "Male"
levels(myData$Gender)[levels(myData$Gender) == "m"] <- "Male"

myData %>% group_by(Gender) %>% summarize(Unique_Values = n()) %>% arrange(desc(Unique_Values))
```

This looks better but there are still suspect values.  We will remove:

- All the `Gender` value counts that are small will be removed.  This affects everything from `U` and below in the table above.
- It is reasonable to assume `Gender` is an informative variable in churn modeling.  Valorem will initially remove the `NULL` values from `Gender`. 

```{r}
myData <- filter(myData, Gender == "Female" | Gender == "Male")
myData$Gender <- factor(myData$Gender)

ggplot(myData, aes(fct_infreq(Gender))) + geom_bar() + xlab("ClubReady Subset - Gender") +
  scale_y_continuous(labels = scales::comma)
```

#### User Factor - UserType

```{r}
myData %>% group_by(UserType) %>% summarize(Unique_Values = n()) %>% arrange(desc(Unique_Values))
ggplot(myData, aes(fct_infreq(UserType))) + geom_bar() + xlab(paste("ClubReady Subset - ", names(myData)[2])) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + scale_y_continuous(labels = scales::comma)
```

ClubReady confirmed Valorem to use:

- ClubClient     
- DeletedClubClient
- ClubClientTemporary

```{r}
myData <- filter(myData, UserType == 'ClubClient' | UserType == 'DeletedClubClient' | UserType == 'ClubClientTemporary')
myData$UserType <- factor(myData$UserType)

ggplot(myData, aes(fct_infreq(UserType))) + geom_bar() + xlab("ClubReady Subset - UserType") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + scale_y_continuous(labels = scales::comma)
```

### Numerical Data

> Because of the large number of `User` records, a random sample is selected in the code below.

```{r}
myNumSum <- numSummary(sample_frac(myData, .3))[, c(1,7,8,16,17)]
myNumSum <- tibble::rownames_to_column(myNumSum)
names(myNumSum)[5] <- "missPCT"
names(myNumSum)[1] <- "Variable_Name"
myNumSum <- arrange(myNumSum, desc(missPCT))
head(myNumSum, 20)
```

No missing data is in the working dataset.

#### Variance

```{r}
#Do not include UserId, StoreId, Gender, UserType, TotalSpent
myVariance <- as.data.frame(apply(myData[,-c(1,2,4,5,41)], 2, var))
myVariance <- tibble::rownames_to_column(myVariance)
names(myVariance)[2] <- "Variance"
myVariance <-  myVariance %>% mutate(Variance2 = ifelse(Variance == 0, "No", "Yes"))
table(myVariance$Variance2)
```

Because `r table(myVariance$Variance2)[1]` variables have no variance - all the values are the same, they can be removed from the working dataset.  If there are no differences in a column, it is of no use in the development of an algorithm.  The variables to be removed because there is no variance are:

```{r}
if(table(myVariance$Variance2)[1] > 0){
  VarNames <- myVariance %>% filter(Variance > 0) %>% select(rowname)
  zeroVarNames <- myVariance %>% filter(Variance == 0) %>% select(rowname)
  myData <- myData %>% select(UserId, StoreId, Gender, TotalSpent,  unlist(VarNames))
  zeroVarNames
}
```

#### Outlier Detection

In the working dataset, there is one variable, `TotalSpent` that should be evaluated to identify any potential outlier.  There are many way to visualize outliers.  While boxplots are the most commonly used visualization, because the number of records is large, plotting is not an optimal reporting option - it takes a long time to plot millions of records.

Comparing the opposite ends of `TotalSpent` produces interesting information:

```{r eval=FALSE, echo=FALSE}
out3 <- ggplot(myData, aes(x = "", y = TotalSpent)) + geom_boxplot(outlier.color="red", outlier.shape=8, outlier.size=4) + 
  scale_y_continuous(labels = scales::dollar)
```

```{r}
tmpRevDesc <- arrange(myData, desc(TotalSpent)) %>% select(TotalSpent)
tmpRevDesc <- tmpRevDesc[1:25,]
tmpRevDesc <- as.data.frame(scales::dollar(tmpRevDesc$TotalSpent))
names(tmpRevDesc) <- "Total_Spent"

tmpRevAsc <- arrange(myData, TotalSpent) %>% select(TotalSpent)
tmpRevAsc <- tmpRevAsc[1:25,]
tmpRevAsc <- as.data.frame(scales::dollar(tmpRevAsc$TotalSpent))
names(tmpRevAsc) <- "Total_Spent"

knitr::kable(list(tmpRevDesc, tmpRevAsc))
```

The highest `TotalSPent` value is `[r tmpRevDesc[1,]` and the lowest value is `r tmpRevAsc[1,]`. 

> It appears the large values may be associated with account credits.  This may be a data quality issue.

#### Duplication

```{r}
cat("The number of duplicated rows is", nrow(myData) - nrow(unique(myData)))
if((nrow(myData) - nrow(unique(myData)))>0) myData[duplicated(myData),]
```

```{r eval=FALSE}
if(nrow(myData) - nrow(unique(myData)) > 0){
  head(myData[duplicated(myData),])
  myData <- myData[!duplicated(myData),]
}
```

Good news - no duplicate records.

### Data Glimpse

Here is what the resulting working dataset looks like.  We are left with `r nrow(myData)` records and `r ncol(myData)`.

```{r}
glimpse(myData)
```

# Initial Data Modeling

This section walks through the Initial Data Modeling phase to identify candidate algorithms to predict different churn models for ClubReady.  The section is organized as follows:

- One of two CSV data files for modelling, `stores.csv`, is explored with detail in the first section
- In the following sections, the other data file (Users) is explored albeit with less explanatory content
- All of these sections roughly follow the same thought process:
    - Develop modeling data structure
    - Create modeling data file and import to Azure Machine Learning Studio
    - Perform Test/Training Data Split
    - Determine "Optimal" Initial Model
  
All of the SQL used to develop these datasets can be found in the `[ML].[ChurnScripts]` stored procedure within the `[Reports]` database.

## Stores

### Develop Modeling Data Structure

#### What is Churn?

During the On-Site Workshop, ClubReady provided a basic definition of *Store Churn*.  If a Store recognizes at least an 80% drop in Revenue over the course of a single month, then the Store is considered to be *Churned*.  

We then need to answer *What is Revenue?*  ClubReady provided the definition that *Revenue* is the total dollars invoiced to the Store by ClubReady.  This information is available in the `[PurchaseLog]` table within the `[ClubReady]` database.  Within this table, we utilize `[StoreId]`, `[PaymentMade]` and `[PurchaseAmount]` columns to determine when the store has *churned*.

A single store cannot churn more than once within our time frame - a two-year time frame starting in November 2015 and ending in October 2017.

#### Dataset Granularity

The next important consideration is the *granularity* of the modelling dataset.  *Granularity* defines what each row in the dataset represents.  In general, the granularity of the modelling dataset should match the data **as it is intended to be used in production.**  During the On-Site Workshop, this was discussed.  The intention is to receive some type of notification that a store is likely to churn within the next few months.  Therefore, a dataset where each row describes a single store for a single month and whether that store churned within that month or within the next few months is required.  This allows ClubReady to run a monthly reporting or notification system that alerts them which stores are at risk of churning.

### Create Modeling Data File {#storemodelingdata}

Using the information from the On-Site Workshop combined with what we learned in the Exploratory Data Analysis phase, the modeling dataset is created.  The following subject areas within the ClubReady database were identified:

- Store Info (Status, Age)
- SMS
- Amenities
- Integrations
- Forms
- Checkins
- Purchased Sessions
- Revenue
- Active Users
- Active Employees
- Classes
- Services

The goal is to create a monthly "snapshot" of each store showing information about that store at that point in time.  However, not all of these subject areas can be queried historically.  This means that is no option but to take the data as it is today, assuming that it has not changed over time.  

The following subject areas do not track historic information:

- Store info (Status)
- SMS
- Amenities
- Integrations
- Forms
- Classes
- Services

This leaves the following subject areas with historic information:

- Store Info (Age)
- Checkins
- Purchased Sessions
- Revenue
- Active Users
- Active Employees

To empower the models using time intelligence, metrics for these subject areas over the past three months were included.  For instance, there are variables for `[TotalRevenue_Past3Months]` and `[ActiveMembers_Past3Months]`.

This was all combined into a single dataset.  The queries to create the `Stores.csv` file can be found in the `[ML].[ClubChurn]` and `[ML].[CompleteStoreData]` stored procedures in the `[Reports]` database.

### Perform Test/Training Data Split {#storesplit}

Similar to the "Modeling Data Structure" phase, the training and testing data should represent a **production use case** as closely as possible.  ClubReady wants to be able to identify whether a store is likely to churn in the next few months.  In this instance, three months was selected as the time interval.  Since the last month in the dataset is October 2017, the testing month should be August 2017.  The records for August 2017 contain information about the month of August, as well as the previous two months for the `Past3Months` variables.  These records also contain a variable `[IsChurn_3Months]` describing whether the store will churn in the next three months.  This is what the model attempts to predict.

In machine learning, it is important the model be tested using data that was not used to train it.  For instance, if a store churns in August 2017, then that will be reflected in the `[IsChurn_3Months]` variable for the months of June and July as well. 

The following training/testing split was developed:

- Training Set 1
    - Start Month: January 2016
    - End Month: May 2017

- Testing Set 1
    - Start Month: August 2017
    - End Month: August 2017

- Variable to Predict: `IsChurn_3Months`

For additional testing, other splits were created: 

- Training Set 2
    - Start Month: January 2016
    - End Month: April 2017

- Testing Set 2
    - Start Month: July 2017
    - End Month: July 2017

- Training Set 3
    - Start Month: January 2016
    - End Month: March 2017

- Testing Set 3
    - Start Month: June 2017
    - End Month: June 2017

### Determine "Optimal" Initial Model

The Initial Modeling phase has two goals.

1. Determine how much of a "pattern" is easily accessible within the data.  For instance, if the best model created in this phase falls substantially below what we would consider "reasonable", then it may decided more or different data is required. 

2. If patterns exist, then use the information to point towards the optimal way to improve the model using feature engineering or additional model optimization.

#### Selecting an Evaluation Metric

The selection of an evaluation metric is one of the most crucial aspects of the data modelling process.  Recognizing that we are in a Binary Classification situation, we are limited to two main contenders, Accuracy/AUC and Precision/Recall.  In general, Accuracy/AUC are acceptable when the two classes, "Churn" and "Not Churn" in this case, are approximately equally likely.  However, in cases where one class is substantially more likely than another, Precision/Recall is the standard.

```{r}
finalstoredata <- read_csv("C:/Users/cweaver/Downloads/Stores_2017_12_11.csv", col_types = cols())

churn <- finalstoredata["IsChurn_3Months"] %>% 
    group_by(IsChurn_3Months) %>%
    summarise(count=n()) %>% 
    mutate(perc=count/sum(count))

ggplot(churn, aes(x = factor(IsChurn_3Months), y = perc*100)) + geom_bar(stat="identity") + ggtitle("IsChurn_3Months") + xlab("IsChurn_3Months") + ylab("Percentage") + geom_text(aes(x = factor(IsChurn_3Months), y = perc*100, label = paste(round(perc*100,0),"%")), nudge_y = 4)
```

Since `[IsChurn_3Months]` variable is heavily imbalanced, Precision/Recall will be used as the performance metric.  Precision is the percentage of correct Churn predictions out of all Churn predictions.  Conversely, Recall is the percentage of correct Churn predictions out of all actual Churn records.  Recall tells how many of the actual Churns being predicted while Precision shows  how many of the predictions are actually Churns.  Both of these metrics tell different stories and are very important.   Maximizing both of them is ideal.  Therefore, an evaluation metric of Precision * Recall is used.

### The "Kitchen Sink" Approach

Azure Machine Learning Studio contains fifteen distinct Binary Classification algorithms.  Fourteen of these can utilize a module called "Tune Model Hyper-parameters".  This module will allow us to train and test multiple machine learning algorithms using different sets of hyper-parameters.  Therefore, using all fourteen modules, it is possible to get insights quickly and easily.  The process looks like this:

![](./KitchenSink.jpg)

The full results of this process can be found in the `Store Churn Data Modelling Results.xlsx` file. Here are the top three models for predicting Store Churn:

| Model Family | Hyper-parameters | Precision | Recall |
| --------------------- | ------------------------------------------ | ------- | -------| 
| Boosted Decision Tree | Leaves: 54 ~ Minimum Instances: 19 ~ Learning Rate: 0.336396 ~ Trees: 51 | 89.3% | 38.4% |
| Boosted Decision Tree | Leaves: 5 ~ Minimum Instances: 37 ~ Learning Rate: 0.030064 ~ Trees: 362 | 100.0% | 30.8% |
| Boosted Decision Tree | Leaves: 6 ~ Minimum Instances: 15 ~ Learning Rate: 0.356604 ~ Trees: 482 | 95.2% | 30.8% |

These results are promising.  There is evidence there is a pattern within the data that may be used to predict when a Store is likely to churn.

## Users

### Develop Modeling Data Structure

#### What is Churn?

During the On-Site Workshop, ClubReady provided a definition of `User Churn`.  If a User's contract expires and is not renewed by the end of the next month, then that User has `churned`.  This information is available in the `[ContractPurchases]` table within the `[ClubReady]` database.  Within this table, `[UserId]`, `[ActivationDateUTC]`, `[AgreedDate]` and `[Cancelled`] columns are used.

Similar to Store Churn, a single user to cannot churn more than once within the time frame.

#### Dataset Granularity

The granularity of the User Churn data is per user, per month, for every month that the user has an active contract, plus an additional record for the month after their contract ends.  This is the "Churn" month.

### Create Modeling Data File

Using the information from the On-Site Workshop combined with what was learned in the Exploratory Data Analysis phase, a User modeling dataset can be developed.  The following subject areas within the ClubReady database were identified:

- Store Churn Data
- User Info (Gender, Type, CheckinCredentialsModified)
- Amenities
- Tags
- Checkins
- Forms
- Purchased Sessions
- Spend

The goal is to create a monthly "snapshot" of each user showing information about that user at that point in time.  However, not all of these subject areas can be queried historically.  

The following subject areas do not track historic information:

- Store Churn Data (see [previous section](#storemodelingdata))
- User Info (Gender, Type, CheckinCredentialsModified)
- Amenities
- Tags
- Forms

This leaves the following subject areas with historic information:

- Store Churn Data (see [previous section](#storemodelingdata))
- Checkins
- Purchased Sessions
- Spend

To empower the models using time intelligence, additional variables for `[TotalSpend_Past3Months]` and `[Checkins_Past3Months]` were created.

All of this information was combined into a single dataset.  The queries to create the `User.csv` file can be found in the `[ML].[MemberChurn]` and `[ML].[CompleteUserData]` stored procedures in the `[Reports]` database.

### Perform Test/Training Data Split

The logic for the test/training split is identical to that of "Store Churn".

- Training Set 1
    - Start Month: January 2016
    - End Month: May 2017

- Testing Set 1
    - Start Month: August 2017
    - End Month: August 2017

- Variable to Predict: `IsChurn_3Months`

For additional testing, we can also move backwards any number of months to create new training/testing splits as follows:

- Training Set 2
    - Start Month: January 2016
    - End Month: April 2017

- Testing Set 2
    - Start Month: July 2017
    - End Month: July 2017

- Training Set 3
    - Start Month: January 2016
    - End Month: March 2017

- Testing Set 3
    - Start Month: June 2017
    - End Month: June 2017

### Determine "Optimal" Initial Model

#### Selecting an Evaluation Metric

A major difference between the Store and User data is the overall size of the data.  The `[FinalUserData]` table is 24GB as a CSV.  Randomly selected sets of 1M, 5M, 10M and 20M rows were created

```{r message=FALSE, warning=FALSE}
finaluserdata <- read_csv("C:/Users/cweaver/Downloads/Users_1M_2017_12_12.csv", col_types = cols(
    Store_UserEmployeeRatio = col_double(),
    Store_UserEmployeeRatio_Last3Months = col_double(),
    Store_AverageMinsPerClass_Service = col_double(),
    Store_AveragePricePerClass_Service = col_double(),
    Store_AverageCancellationHrsPerClass_Service = col_double(),
    Store_AverageRescheduleDeadlinePerClass_Service = col_double(),
    Store_AverageMinsPerClass = col_double(),
    Store_AveragePricePerClass = col_double(),
    Store_AverageCancellationHrsPerClass = col_double(),
    Store_AverageRescheduleDeadlinePerClass = col_double(),
    Store_AverageMinsPerService = col_double(),
    Store_AveragePricePerService = col_double(),
    Store_AverageCancellationHrsPerService = col_double(),
    Store_AverageRescheduleDeadlinePerService = col_double())
    ,progress = FALSE
)

churn <- finaluserdata["IsChurn_3Months"] %>% group_by(IsChurn_3Months) %>% summarise(count=n()) %>% mutate(perc=count/sum(count))

ggplot(churn, aes(x = factor(IsChurn_3Months), y = perc*100)) + geom_bar(stat="identity") + ggtitle("IsChurn_3Months") + xlab("IsChurn_3Months") + ylab("Percentage") + geom_text(aes(x = factor(IsChurn_3Months), y = perc*100, label = paste(round(perc*100,0),"%")), nudge_y = 4)
```

### The "Kitchen Sink" Approach

The full results of this process can be found in *User Churn Data Modelling Results.xlsx* file. Here are the top three models for predicting Store Churn:

| Model Family | Hyper-parameters | Precision | Recall |
| --------------------- | ------------------------------------------ | ------- | -------| 
| Boosted Decision Tree | Leaves: 32 ~ Minimum Instances: 6 ~ Learning Rate: 0.252098531 ~ Trees: 270 | 57.1% | 16.5% |
| Boosted Decision Tree | Leaves: 54 ~ Minimum Instances: 19 ~ Learning Rate: 0.336396247 ~ Trees: 51 | 55.0% | 15.9% |
| Boosted Decision Tree | Leaves: 17 ~ Minimum Instances: 13 ~ Learning Rate: 0.06286619 ~ Trees: 50 | 54.5% | 12.6% |

The Precision and Recall of these metrics are quite low.  Other methods are required to develop a feasible model - if the data allows an algorithmic solution.

### Further Investigation

The datasets used to develop the initial data models contained 220 predictors and either 1, 5 or 10 million records.  Each dataset provided similar results.  Therefore, it may be worth considering ways to reduce the number of predictors in the model.  It is possible 220 predictors with a large number of zeros is simply overwhelming the modeling algorithms leading to poor models.  Therefore, reducing the number of predictors without losing much information, may improve algorithm performance.  A common technique for this is Principal Components Analysis (PCA).  Before PCA can be performed, the dataset must be cleaned by removing non-predictive columns.

```{r}
ignore.set <- c("UserId", "StoreId", "ReferenceMonthStartDate","IsChurn", "IsChurn_2Months", "IsChurn_3Months", "IsChurn_6Months")
disp <- data.frame(ignore.set)
names(disp) <- "Variables Ignored for Business Reasons"
disp
```

Next, remove any variables with missing values, as PCA does not allow for these.  

```{r}
finaluserdata.miss <- finaluserdata[,-which(names(finaluserdata) %in% ignore.set)]
myNumSum <- numSummary(finaluserdata.miss)[, c(1,7,8,16,17)]
myNumSum <- tibble::rownames_to_column(myNumSum)
names(myNumSum)[5] <- "missCNT"
names(myNumSum)[1] <- "Variable_Name"
myNumSum <- arrange(myNumSum, desc(n))
ignore.miss <- myNumSum[myNumSum[,5] > 0,1]
disp <- data.frame(ignore.miss)
names(disp) <- c("Variables with Missing Values")
disp
```

Remove variables that have no variance, as these are not useful for predictive modeling

```{r, warning=FALSE,message=FALSE}
finaluserdata.var <- finaluserdata.miss[,-which(names(finaluserdata.miss) %in% ignore.miss)]
ignore.var <- names(which(round(apply(finaluserdata.var, MARGIN = 2, FUN = var), 4)<= 0))
disp <- data.frame(ignore.var)
names(disp) <- "Variables with No Variance"
disp
```

Perform *One-Hot Processing* to change remaining categorical variables into numeric ones, a PCA requirement.

> The output from this code is excessive so the results are not shown.

```{r results='hide'}
finaluserdata.enc <- finaluserdata.var[,-which(names(finaluserdata.var) %in% ignore.var)]
ignore.char <- row.names(charSummary(finaluserdata.enc))
encoder <- onehot(finaluserdata.enc[,which(names(finaluserdata.enc) %in% ignore.char)], stringsAsFactors = TRUE)
dat.encoded <- predict(encoder, finaluserdata.enc)
finaluserdata.encoded <- data.frame(finaluserdata.enc[,-which(names(finaluserdata.enc) %in% ignore.char)], dat.encoded)
head(finaluserdata.encoded)
```

Perform PCA to reduce our data to a more manageable number of variables.  Start by looking at the percentage of total *information* contained within each of the new variables, known as principal Components.

The results of the first 13 ordered descending records are shown.

```{r}
pca <- princomp(finaluserdata.encoded)
info <- pca$sdev / sum(pca$sdev)
disp <- data.frame(paste(round(info*100,1), "%"))
names(disp) <- "Information Percentage by Component"
row.names(disp) <- names(info)
head(disp, 15)
```

This highlights a significant reason why the User Churn models were not very effective.  The 199 predictors evaluated by PCA could be nearly entirely replaced by a few dense variables.  In other words, the data did not contain very much "information".  

```{r}
info.cum <- cumsum(info)
disp <- data.frame(paste(round(info*100,1), "%"))
names(disp) <- c("Cumulative Information Percentage by Component")
rownames(disp) <- names(info.cum)
disp
head(disp, 15)
```

It appears only the first five components are needed.  Use this information in Azure Machine Learning Studio to rerun the Initial Data Modeling step using the new dataset containing the five principal components, as well as the columns containing the missing data, as they could still provide value.  Here are the results:

**Original Dataset (220 Predictors)**

| Model Family | Hyper-parameters | Precision | Recall |
| --------------------- | ------------------------------------------ | ------- | -------| 
| Boosted Decision Tree | Leaves: 32 ~ Minimum Instances: 6 ~ Learning Rate: 0.252098531 ~ Trees: 270 | 57.1% | 16.5% |
| Boosted Decision Tree | Leaves: 54 ~ Minimum Instances: 19 ~ Learning Rate: 0.336396247 ~ Trees: 51 | 55.0% | 15.9% |
| Boosted Decision Tree | Leaves: 17 ~ Minimum Instances: 13 ~ Learning Rate: 0.06286619 ~ Trees: 50 | 54.5% | 12.6% |

**PCA Dataset (12 Predictors + 5 Principal Components)**

| Model Family | Hyper-parameters | Precision | Recall |
| --------------------- | ------------------------------------------ | ------- | -------| 
| Neural Network - Gaussian Normalizer | Learning Rate: 0.034618 ~ Loss Function: Cross Entropy ~ Iterations: 29 | 15.0% | 2.0% |
| Neural Network - Gaussian Normalizer | Learning Rate: 0.030355 ~ Loss Function: Cross Entropy ~ Iterations: 27 | 18.2% | 1.5% |
| Neural Network - Binning Normalizer | Learning Rate: 0.037861 ~ Loss Function: Cross Entropy ~ Iterations: 129 | 20.3% | 1.1% |

Using PCA on this dataset is not beneficial.  

# Feature Selection

This section walks through the Feature Selection phase to determine which variables are important to the models we identified in the Initial Data Modeling phase.

- One of two models, `Stores`, will be examined in the first section.
- In the following sections, the other model, `Users` is explored, albeit with less explanatory content.

## Stores

It is known there are patterns in the `Stores` data to predict whether a Store will churn in the next three months.  One concern with the current models is that they leverage over 100 variables.  This could be troublesome to operationalize.  Therefore, the next step is to identify which of the variables are important to the model.  Since a model has been created, a technique known as permutation feature importance may be leveraged.  

![](./PermutationFeatureImportance.jpg)

The Permutation Feature Importance module in Azure Machine Learning does not support the custom Precision * Recall metric.  Therefore, run the module twice, once for Precision and once for Recall.  Then determine which variables do not provide value for Precision or Recall.  The results can be found in the `Store Churn Data Modeling Results.xlsx` file.  Here is a summary:

| Variable Name | Precision PFI | Recall PFI |
| ------------- | ------------- | ---------- | 
| TotalRevenue | 0.754623 | 0.153846 |
| TotalRevenue_Past3Months | 0.474503 | 0 |
| Status | 0.142292 | 0.061538 |
| Checkins_Evening | 0.100334 | 0 |
| Checkins_Hour6to7_Past3Months | 0.036232 | 0 |
| Checkins_Hour11to12_Past3Months | 0.036232 | 0 |
| ActiveUsers_Last3Months | 0.036232 | 0 |
| AverageMinsPerClass_Service | 0.036232 | 0 |
| Services | 0.036232 | 0 |
| Checkins_Hour8to9 | 0.023411 | 0 |
| Integration_Listen360 | 0.012422 | 0.030769 |
| Forms | 0.012422 | 0.030769 |
| Checkins_Monday | 0.005929 | 0.015385 |
| Checkins_Weekday | 0.005929 | 0.015385 |
| Checkins_Hour14to15 | 0.005929 | 0.015385 |
| Checkins_Morning | 0.005929 | 0.015385 |
| Checkins_Afternoon | 0.005929 | 0.015385 |
| Checkins_Monday_Past3Months | 0.005929 | 0.015385 |
| Checkins_Hour12to13_Past3Months | 0.005929 | 0.015385 |
| Checkins_Evening_Past3Months | 0.005929 | 0.015385 |
| ActiveEmployees_Last3Months | 0.005929 | 0.015385 |
| UserEmployeeRatio_Last3Months | 0.005929 | 0.015385 |
| ActiveEmployees | 0 | 0.015385 |

Any variables not included did not show any importance for either metric.  Zeroes are not actually zeroes - they represent very small numbers.  When variable size is reduced, the performance of the metrics is reduced.  This is seen below.

**Full Variable Set**

| Test/Training Set | Precision | Recall | Precision * Recall |
| ----------------- | --------- | ------ | ------------------ |
| Set 1 | 0.893 | 0.384 | 0.343 |

**Precision/Recall Variable Set**

| Test/Training Set | Precision | Recall | Precision * Recall |
| ----------------- | --------- | ------ | ------------------ |
| Set 1 | 0.815 | 0.336 | 0.27384 |
| Set 2 | 0.767 | 0.287 | 0.220129 |
| Set 3 | 0.84 | 0.309 | 0.25956 |

Another analysis was performed including other evaluation metrics, Accuracy and Average Log Loss.

| Variable Name | Precision PFI | Recall PFI | Accuracy PFI | Average Log Loss PFI |
| ------------------------------- | ------- | ------- | ------- | ------- |
| TotalRevenue | 0.754623 | 0.153846 | 0.041257 | 0.345677 |
| TotalRevenue_Past3Months | 0.474503 | 0 | 0.016699 | 0.032497 |
| Status | 0.142292 | 0.061538 | 0.003438 | 0.08103 |
| Checkins_Evening | 0.100334 | 0 | 0.001473 | 0 |
| Checkins_Hour6to7_Past3Months | 0.036232 | 0 | 0.000491 | 0 |
| Checkins_Hour11to12_Past3Months | 0.036232 | 0 | 0.000491 | 0 |
| ActiveUsers_Last3Months | 0.036232 | 0 | 0.000491 | 0.007357 |
| AverageMinsPerClass_Service | 0.036232 | 0 | 0.000491 | 0.001426 |
| Services | 0.036232 | 0 | 0.000491 | 0 |
| Checkins_Hour8to9 | 0.023411 | 0 | 0 | 0 |
| Integration_Listen360 | 0.012422 | 0.030769 | 0.000982 | 0.005943 |
| Forms | 0.012422 | 0.030769 | 0.000982 | 0.004854 |
| Checkins_Monday | 0.005929 | 0.015385 | 0.000491 | 0.005731 |
| Checkins_Weekday | 0.005929 | 0.015385 | 0.000491 | 0.002713 |
| Checkins_Hour14to15 | 0.005929 | 0.015385 | 0.000491 | 0.003422 |
| Checkins_Morning | 0.005929 | 0.015385 | 0.000491 | 0.003546 |
| Checkins_Afternoon | 0.005929 | 0.015385 | 0.000491 | 0.002234 |
| Checkins_Monday_Past3Months | 0.005929 | 0.015385 | 0.000491 | 0.000259 |
| Checkins_Hour12to13_Past3Months | 0.005929 | 0.015385 | 0.000491 | 0 |
| Checkins_Evening_Past3Months | 0.005929 | 0.015385 | 0.000491 | 0.00133 |
| ActiveEmployees_Last3Months | 0.005929 | 0.015385 | 0.000491 | 0 |
| UserEmployeeRatio_Last3Months | 0.005929 | 0.015385 | 0.000491 | 0 |
| ActiveEmployees | 0 | 0.015385 | 0 | 0.000637 |
| PurchasedSessions | 0 | 0 | 0 | 0.009245 |
| Amenities | 0 | 0 | 0 | 0.007819 |
| StoreAgeMonths | 0 | 0 | 0 | 0.003792 |
| UserEmployeeRatio | 0 | 0 | 0 | 0.002884 |
| Checkins_Thursday_Past3Months | 0 | 0 | 0 | 0.002714 |
| Checkins | 0 | 0 | 0 | 0.002576 |
| Checkins_Hour4to5 | 0 | 0 | 0 | 0.00185 |
| Checkins_Past3Months | 0 | 0 | 0 | 0.001458 |
| Checkins_Hour9to10 | 0 | 0 | 0 | 0.001359 |
| AveragePricePerService | 0 | 0 | 0 | 0.000855 |
| AveragePricePerClass_Service | 0 | 0 | 0 | 0.000808 |
| Checkins_Hour6to7 | 0 | 0 | 0 | 0.000788 |
| Checkins_Hour1to2_Past3Months | 0 | 0 | 0 | 0.000597 |
| Checkins_Hour13to14 | 0 | 0 | 0 | 0.000224 |
| NonRequiredWaiver | 0 | 0 | 0 | 0.000083 |
| Checkins_Hour17to18 | 0 | 0 | 0 | 0.000071 |
| Checkins_Saturday | 0 | 0 | 0 | 0.000013 |

If all of these variables are used, there is some improvement moving closer to the original model.

**Full Variable Set (130 Variables)**

| Test/Training Set | Precision | Recall | Precision * Recall |
| ----------------- | --------- | ------ | ------------------ |
| Set 1 | 0.893 | 0.384 | 0.343 |

**Precision/Recall Variable Set (23 Variables)**

| Test/Training Set | Precision | Recall | Precision * Recall |
| ----------------- | --------- | ------ | ------------------ |
| Set 1 | 0.815 | 0.336 | 0.27384 |
| Set 2 | 0.767 | 0.287 | 0.220129 |
| Set 3 | 0.84 | 0.309 | 0.25956 |

**Precision/Recall/Accuracy/Average Log Loss Variable Set (40 Variables)**

| Test/Training Set | Precision | Recall | Precision * Recall |
| ----------------- | --------- | ------ | ------------------ |
| Set 1 | 0.88 | 0.338 | 0.29744 |
| Set 2 | 0.889 | 0.3 | 0.2667 |
| Set 3 | 0.96 | 0.353 | 0.33888 |

This looks promising.  Given the Users data contains all of these Store variables, we will wait until we see the Permutation Feature Importance results for the User Churn scenario before we decide which variables we will keep.

# Survival Analysis

## Introduction

In the Initial Data Modeling phase for User Churn, the data set does not contain sufficient information for Classification.  However, there are other approaches to this type of problem.  One such alternative is Survival Analysis.

In Survival Analysis, the goal is to predict *Time to Failure*.  In order to do this, definition for *Start Time* and *End Time* must be defined.  The *End Time* is obvious, it is the first day of the Churn Month for that User.  For simplicity, it was decided to use the first day of the `Activation Month` for that User as the *Start Time*.

In this Survival Analysis, the *Cox Proportional Hazards* model is used.  This model allows passing in a number of numeric variables for each combination of month and user.  The model will predict how may months until the User is expected to churn.

### Data Preparation

To prepare our data for the Cox Proportional Hazards model, the dataset must be changed slightly.  Instead of using the `IsChurn`, create a new set of variables called `MonthsUntilChurn`.  The SQL for creating this dataset can be found in the `[ML].[SA_CompleteUserData]` stored procedure in the `[Reports]` database.

Below is the survival curve for the sampled dataset.

```{r, warning=FALSE,message=FALSE}
sauserdata <- read_csv("C:/Users/cweaver/Downloads/Users_SA_1M_2017_12_29.csv", col_types = cols(
    Store_UserEmployeeRatio = col_double(),
    Store_UserEmployeeRatio_Last3Months = col_double(),
    Store_AverageMinsPerClass_Service = col_double(),
    Store_AveragePricePerClass_Service = col_double(),
    Store_AverageCancellationHrsPerClass_Service = col_double(),
    Store_AverageRescheduleDeadlinePerClass_Service = col_double(),
    Store_AverageMinsPerClass = col_double(),
    Store_AveragePricePerClass = col_double(),
    Store_AverageCancellationHrsPerClass = col_double(),
    Store_AverageRescheduleDeadlinePerClass = col_double(),
    Store_AverageMinsPerService = col_double(),
    Store_AveragePricePerService = col_double(),
    Store_AverageCancellationHrsPerService = col_double(),
    Store_AverageRescheduleDeadlinePerService = col_double())
    ,progress = FALSE
)

sa <- Surv(sauserdata$MonthsUntilChurn)
sa.fit <- survfit(sa ~ 1)
plot(sa.fit, main = "User Churn Survival Curve", xlab = "Months Active", ylab = "Proportion of Users Still Active")
```

Following the same process explore previously, remove non-predictive variables - 

```{r, warning=FALSE,message=FALSE}
ignore.set <- c("UserId", "StoreId", "ReferenceMonthStartDate","IsChurn", "IsChurn_2Months", "IsChurn_3Months", "IsChurn_6Months", "MonthsUntilChurn_RC")
disp <- data.frame(ignore.set)
names(disp) <- "Variables Ignored for Business Reasons"
disp
```

Remove any variables with missing values -

```{r, warning=FALSE,message=FALSE}
sauserdata.miss <- sauserdata[,-which(names(sauserdata) %in% ignore.set)]
myNumSum <- numSummary(sauserdata.miss)[, c(1,7,8,16,17)]
myNumSum <- tibble::rownames_to_column(myNumSum)
names(myNumSum)[5] <- "missCNT"
names(myNumSum)[1] <- "Variable_Name"
myNumSum <- arrange(myNumSum, desc(n))
ignore.miss <- myNumSum[myNumSum[,5] > 0,1]
disp <- data.frame(ignore.miss)
names(disp) <- c("Variables with Missing Values")
disp
```

Removed variables with no variance - 

```{r, warning=FALSE,message=FALSE}
sauserdata.var <- sauserdata.miss[,-which(names(sauserdata.miss) %in% ignore.miss)]
ignore.var <- names(which(round(apply(sauserdata.var, MARGIN = 2, FUN = var), 4)<= 0))
disp <- data.frame(ignore.var)
names(disp) <- "Variables with No Variance"
disp
```

Perform *One-Hot Processing* to change any remaining categorical variables into numeric ones.  (Again, the results are excessive and not displayed.) 

```{r, warning=FALSE,message=FALSE, results='hide'}
sauserdata.enc <- sauserdata.var[,-which(names(sauserdata.var) %in% ignore.var)]
ignore.char <- row.names(charSummary(sauserdata.enc))
encoder <- onehot(sauserdata.enc[,which(names(sauserdata.enc) %in% ignore.char)], stringsAsFactors = TRUE)
dat.encoded <- predict(encoder, sauserdata.enc)
sauserdata.encoded <- data.frame(sauserdata.enc[,-which(names(sauserdata.enc) %in% ignore.char)], dat.encoded)
glimpse(sauserdata.encoded)
```

Create our testing and training sets - 

```{r, warning=FALSE,message=FALSE}
ind.train <- sample(seq_len(dim(sauserdata.encoded)[1]), floor(dim(sauserdata.encoded)[1] * .7))
sauserdata.train <- sauserdata.encoded[ind.train,]
sauserdata.test <- sauserdata.encoded[-ind.train,]
disp <- as.data.frame(matrix(c(dim(sauserdata.train),dim(sauserdata.test)),ncol=2,byrow=FALSE))
names(disp) <- c("Training set", "Testing Set")
row.names(disp) <- c("Number of Records", "Number of Variables")
disp
```

### Cox Proportional Hazards Model

Train the survival analysis predictive model.

```{r, warning=FALSE,message=FALSE}
sa <- Surv(sauserdata.train$MonthsUntilChurn)
cox.form <- formula(paste("sa ~ ", paste(names(sauserdata.train), collapse = " + "), " - MonthsUntilChurn"))
cox.fit <- coxph(cox.form, data = sauserdata.train)
```

Survival Analysis is different than Classification.  In Survival Analysis, binomial predictions are not output.  Instead, survival curves are provided.  Here is a survival curve for one user.

```{r, warning=FALSE,message=FALSE}
cox.test <- survfit(cox.fit, newdata = sauserdata.test)$surv
plot(cox.test[,1], main = "Survival Curve for User", xlab = "Months Active", ylab = "Probability of Survival", type = "l")
abline(a=.5, b=0, col = "red")
```

The curve is used to determine when the User is *likely to churn*.  In other words, when is the probability of the user churning greater than the probability of the user not churning - when does the probability of survival drop below 0.5?

```{r, warning=FALSE,message=FALSE}
pred <- function(x = c(), thresh = .5){
	y <- order(x<=thresh, x, decreasing = c(TRUE, TRUE))[1]
	return(y)
}
sapred <- apply(cox.test, MARGIN = 2, FUN = pred)
saeval <- data.frame(sauserdata.test$MonthsUntilChurn, sapred)
names(saeval) <- c("Actual", "Predicted")
head(saeval)
```

## Conclusion

The survival analysis for User Churn did not provide adequate performance to proceed.  Valorem submits this is further evidence the data for User Churn does not support and algorithmic solution.  More user-centric data is needed.

# NEW Data Analysis

This information below was not originally intended in the project SOW.  It was created as a natural extension of the Digital Insights Workshop to provide a glimpse of what a modern data platform can provide in terms of additional data insights.

## Introduction

In the User Data Modeling phase, it was discovered the `User Churn` data does not contain patterns or signals strong enough to create a useful algorithmic model.    This leads to the question, *What data should we have so we can develop a user churn predictive model?*

Valorem created a Power BI demonstration to help ClubReady answer this question. The demonstration solution is available for review in the *Users Prototype Analysis.pbix* file. This was created to introduce ClubReady to the opportunities for data exploration using the simplicity - and power - of Power BI. 

The analysis that follows can be reproduced using the PBIX file.

## Data

The variables in the `[ClubReady].[dbo].[Users], [UserAmenityHistory], [Tags] and [ContractPurchases]` tables were evaluated to determine which ones were sufficiently populated and contained information with potentially useful analytic value.  The variables include:

- [Users] Activations
- [Users] CheckinCredentialsModified, HasDoctor & HasAltPhone
- [Users] AgeAtActivation & AgeAtChurn
- [Users] MonthsActive
- [Users] Address
- [Users] Referral Type & Prospect Type
- [UserAmenityHistory] AmenityName
- [Tags] Tags
- [ContractPurchases] HasRequiredSignatures, ElectronicSignature & Membership
- [ContractPurchases]PurchasedSessions
- [ContractPurchases]ElectronicSignaturesTaken

The SQL code can be found in the `[PBI].[Users]` view within the `[Reports]` database.

### Activations

![](./images/ChurnandActivationsoverTime.jpg) 

Activations higher than churn is desirable because that means ClubReady is gaining users.  This is the case for the the majority of the time frames shown.  `[ChurnMonthStartDate]` is a potential predictor for User Churn.

Here are a few more trends found by using Power BI:

![](./images/ChurnStep1.jpg)

![](./images/ActivationsStep1.jpg)

### Binary Users Variables

![](./images/UsersStep1.jpg)

`[CheckinCredentialsModified]`, `[HasDoctor]` and `[HasAltPhone]` variables are potentially useful.  However, further inspection shows that `[HasDoctor]` and `[HasAltPhone]` variables are too rarely populated to be of use.  This leaves `[CheckinCredentialsModified]`.

### User Age

Evaluate the User Age variables, `AgeAtActivation` and `AgeAtChurn`.

![](./images/UserAgeHistograms.jpg)

Churn and Activations seem to follow a similar trend with respect to age.  Explore further by examining Churn % by Age.

![](./images/ChurnbyAge.jpg)

There is a clear trend between `Churn` and `AgeAtActivation`.

### Months Active

Does this same relationship hold true for Length of Membership?

![](./images/MonthsActiveHistograms.jpg)

The top histogram shows a very common problem with certain numeric data, such as date-based and monetary data.  This data is generally very right-skewed.  A way is needed to create bins that are not equally sized.  An easy way to do this is with the logarithm transform.  The bottom histogram shows how this creates a much more usable visualization.  There's obviously a relationship between Total Churn and Length of Membership.  Now determine if this holds true for Churn %.

![](./images/ChurnbyMonthsActive.jpg)

Another interesting result!  User Churn within the first year is extremely high.  However, this drops dramatically as they remain active for subsequent years.  `[MonthsActive]` is also a potentially powerful predictor.

### Geography

Evaluate `User Address` on Churn.

![](./images/ChurnbyGeo.jpg)

Looking at the charts on the right, it is easy to see that State, Zip and City show high variability in User Churn.  This could potentially make these fields useful for modeling.  However, it is important to note the size of these variables in the charts on the left.  Given the number of distinct values in these variables, it is unlikely using them directly will lead to much power.  Instead, this may be a use case for some type of count-based or ratio-based variables.  Another thing to consider is that the City names seem very unreliable.  Hesitant to call these "high-quality" predictors without being able to subject them to data cleansing.

### Referral Type and Prospect Type

The final two variables from the Users table are `[Referral Type]` and `[Prospect Type]`. 

![](./images/ChurnbyReferralTypeandProspectType.jpg)
`
Similar to Geos, `[Referral Type]` and `[Prospect Type]` show high variability for User Churn.  However, their high granularity leads us to think that Count-based or Ratio-based variables would be beneficial.

## Amenity

The next dataset we want to examine is `UserAmenityHistory`.  This dataset contains all of the Amenities connected to each user.  This dataset is quite small and only contains one field of interest, `[AmenityName]`.  Examine the impact Amenity selection has on User Churn.

![](./images/ChurnbyAmenity.jpg)

Amenity selection creates variability for User Churn. Use Power BI's built-in interactions to see how this plays out within particular stores.

![](./images/ChurnbyAmenityandStore.jpg)

It is reasonable to conclude `Amenity` is a potential predictor for User Churn, especially in conjunction with Store.  However, given it's high granularity, may need to consider some type of Count-based or Ratio-Based approach.

## Tag

Similar to Amenities, this dataset contains all of the Tags associated to each user. 

![](./images/ChurnbyTag.jpg)

![](./images/ChurnbyTagandStore.jpg)

`Tags` suggest a very similar story to `Amenities`.  However, given the free-form nature of the field and how rarely they are used, it seems unlikely much insight can be derived.  This is supported up by the fact the "Tags" field in the Store Churn dataset and it was thrown out by the Feature Selection process.

## ContractPurchases

Evaluate `ContractPurchases` that contains the membership-related purchases for each customer.  

### Binary ContractPurchases Variables

![](./images/ContractPurchasesStep1.jpg)

`[HasRequiredSignatures]`, `[ElectronicSignature]` and `[Membership]` all show variability for User Churn.  The bottom charts shows these variables are well-populated.  The same is not true for `[CommissionsOk]` and `[SMSCheck]`. 
- HasRequiredSignature = TRUE : ElectronicSignature
- HasRequiredSignature = TRUE : Membership
- ElectronicSignature = TRUE : Membership

It appears `[HasRequiredSignatures]`, `[ElectronicSignature]` and `[Membership]` fields all contain similar information.  This means they are potentially good predictors.  

### Purchased Sessions

The major component of the *ContractPurchases* dataset is `[PurchasedSessions]`.  This informs how active a user is. 

![](./images/PurchasedSessionsHistograms.jpg)

Because `PurchasedSessions` distribution is right-skewed, a log transformation is applied. Logarithm transformation. 

![](./images/ChurnbyPurchasedSessions.jpg)

Interesting.  One way to create useful variables out of counts is to create "buckets", also known as "discretization".  This chart tells us that we could create the following buckets for `[PurchasedSessions]`, possibly leading to a useful predictor.

- 0 PurchasedSessions
- 1 PurchasedSession
- 2 PurchasedSessions
- 3 PurchasedSessions
- 4 - 40 PurchasedSessions
- More than 40 PurchasedSessions

Review Purchased Sessions over Time to see if there is anything useful.

![](./images/PurchasedSessionsoverTime.jpg)

Curious `[PurchasedSessions]` follows the same chronological trend as `Activations`, as opposed to `Churn`.  This is not helpful for modeling, but it does suggest further exploration may be warranted.  Additionally, the scatterplot at shows `[PurchasedSessions]` was positively correlated with `Churn` in 2016 but negatively in `2017`.  Again, this is not helpful for modeling, but it is informative and thought-provoking.  

> It is likely including `PurchasedSessions` and a discretized version in the `User Churn` dataset may help create a better model.

#### Electronic Signatures Taken

The final variable in the `ContractPurchases` dataset is `[ElectronicSignaturesTaken]`.  This variable records the number of signatures required for the user to purchase the contract.  

![](./images/ChurnbyElectronicSignaturesTaken.jpg)

There appears to be a weak correlation between `[ElectronicSignaturesTaken]` and `[Churn %]`.  However, the number of users in each of the segments greater than 0 is small.  Therefore, it may also be beneficial to include a binary version of this variable, `[ElectronicSignaturesTaken>0]`.  Theoretically, this should be the same as the `[ElectronicSignature]` variable.  However, there is a small data quality issue that causes them to be different.  It might be worthwhile to investigate this at a later date.

### Results

Below are all the fields identified in this section as potentially useful for predicting User Churn.  These can be found in the *User Churn Data Analysis Results.xlsx* file.

| Database | Schema | Table | Field | Final Table | Final Field | Notes |
|-----------|-----|-------|-------|-------|-------|--------------------|
| Reports | ML | UserChurn | All | Users |  |  |
| ClubReady | dbo | Users | CheckinCredentialsModified | Users | CheckinCredentialsModified |  |
| ClubReady | dbo | Users | EmergencyContactName | Users | HasEmergencyContact |  |
| ClubReady | dbo | Users | EmergencyContactPhone | Users | HasEmergencyContact |  |
| ClubReady | dbo | Users | Phone | Users | HasPhone |  |
| ClubReady | dbo | Users | CellPhone | Users | HasCellPhone |  |
| ClubReady | dbo | ContractPurchases | ActivationDateUtc | Users | AgeAtActivation |  |
| ClubReady | dbo | ContractPurchases | AgreedDate | Users | AgeAtActivation |  |
| ClubReady | dbo | Users | dob | Users | AgeAtActivation |  |
| ClubReady | dbo | Users | (Reused Field) | Users | MonthsActive |  |
| ClubReady | dbo | Users | State | Users | State | Needs Count-Based or Ratio-Based Approach |
| ClubReady | dbo | Users | Zip | Users | Zip | Needs Count-Based or Ratio-Based Approach |
| ClubReady | dbo | Users | City | Users | City | Needs Count-Based or Ratio-Based Approach; Needs Data Cleansing |
| ClubReady | dbo | Users | ReferralTypeId | Users | Referral Type | Needs Count-Based or Ratio-Based Approach |
| ClubReady | dbo | ReferralType | ReferralType | Users | Referral Type | Needs Count-Based or Ratio-Based Approach |
| ClubReady | dbo | Users | ProspectTypeId | Users | Prospect Type | Needs Count-Based or Ratio-Based Approach |
| ClubReady | dbo | ProspectType | ProspectType | Users | Prospect Type | Needs Count-Based or Ratio-Based Approach |
| ClubReady | dbo | Amenities | AmenityName | UserAmenityHistory | Amenity | Needs Count-Based or Ratio-Based Approach |
| ClubReady | dbo | ContractPurchases | HasRequiredSignatures | ContractPurchases | HasRequiredSignatures |  |
| ClubReady | dbo | ContractPurchases | ElectronicSignature | ContractPurchases | ElectronicSignature |  |
| ClubReady | dbo | ContractPurchases | Membership | ContractPurchases | Membership |  |
| ClubReady | dbo | ContractPurchases | TotalSessions | ContractPurchases | PurchasedSessions | Discretize to [0, 1, 2, 3, 4-40, >40] |
| ClubReady | dbo | ContractPurchases | ElectronicSignaturesTaken | ContractPurchases | ElectronicSignaturesTaken | Discretize to [0, >0] |

### Further Analysis

The tables that could contain potentially useful variables for User Churn are presented below:

- [ClubReady].[dbo].[PurchaseLog]
- [ClubReady].[dbo].[Stores]
- [ClubReady].[dbo].[Checkinlog_v]
- [ClubReady].[dbo].[UserWaiverRequirement]
- [ClubReady].[dbo].[DeclineLog]
- [ClubReady].[dbo].[bookings]
- [ClubReady].[dbo].[loginlog]

# Conclusions & Next Steps

Throughout the Digital Insights Workshop, Valorem and ClubReady collaborated to create a data-driven journey designed to reshape ClubReady into a *data first* organization.

**Store Churn**:  A prototype "Store Churn" algorithm that predicts when a Store is likely to go out of business in the next three months was created.  This is accessible via an Azure Machine Learning Web Service.  It is recommended ClubReady analyze the results of this algorithm for the next few months to determine its real-world accuracy.  If successful, ClubReady may incorporate the algorithm in its application or use it as a standalone utility.  If the algorithm fails to meet expected performance then a targeted analysis to determine it's deficiencies and corrective action could be undertaken.

**User Churn**:  Less success developing a User Churn algorithm was realized.  The data analysis and the modeling strongly suggests the data does not have sufficient user behavioral informative-rich data to build an algorithmic predictive solution.  This was discovered during the user churn modeling and confirmed through PCA Analysis.  Perhaps in the future ClubReady applications will include more user behavioral data so a user churn model can be developed.

**Future Data Exploration**:  The data exploration and associated R code and the resulting Power BI Desktop file can be used as a guide to design a full-scale analytic platform.  This provides ClubReady the opportunity to interactively explore what stories the data might tell thereby unlocking the ability to not only answer questions, but also to understand which questions have yet to be asked.  This contributes to faster, accurate and more efficient reporting opportunities.  This is a critical step to becoming a *data first* organization.

**Recommendation - Transaction Platform Redesign**: ClubReady uses a single Azure IaaS SQL Database to power its application.  This is the product of years of agile development and rapidly implemented business features - often customer-specific changes.  With the availability of newer technologies, such as Data Lake, this is an opportune time to consider a redesign of the existing system.  This will open up new opportunities for more agile and/or continuous integration and development.  It will also strengthen ClubReady's ability to perform routine and ad hoc analyses as part of its Data Analytics journey.  This builds the environment where machine learning can become the fabric stitching the data and evolving business requirements into a industry-leading data-first platform.
