---
title: "Microsoft O365 bayesAB Example 2"
output:
  rmdformats::readthedown:
    highlight: pygments
    code_folding: hide
---

<style type="text/css">
p{ /* Normal  */
   font-size: 12px;
}
body{ /* Normal  */
   font-size: 12px;
}
td {  /* Table  */
   font-size: 10px;
}
h1 { /* Header 1 */
 font-size: 26px;
 color: #4294ce;
}
h2 { /* Header 2 */
 font-size: 22px;
}
h3 { /* Header 3 */
 font-size: 18px;
}
code.r{ /* Code block */
  font-size: 10px;
}
pre { /* Code block */
  font-size: 10px
}
#table-of-contents h2 {
background-color: #4294ce;
}
#table-of-contents{
background: #688FAD;
}
#nav-top span.glyphicon{
color: #4294ce;
}
#postamble{
background: #4294ce;
border-top: ;
}
</style>

```{r}
Beta_Parameters <- function(mean, variance) {
  alpha <- ((1 - mean) / variance - 1 / mean) * mean ^ 2
  beta <- alpha * (1 / mean - 1)
  return(Beta_Parameters = list(alpha = alpha, beta = beta))
}
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("dplyr", "ggplot2", "bayesAB", "gridExtra", prompt = FALSE)
```

# Use Case

Imagine an email campaign where there are 3 differently formatted emails (A, B, and C).  The goal is to determine which is most effective.

# Get Data

The most experienced suggest that historically, features in ads B and C would perfom better than A.  Also, C is likely to outperform B.  Specifically:

- Click rate for C to follow a bell curve with the mean of 17% and standard deviation of 4%
- Expect A and B to have the respective mean click rates of 15% and 16%. They also have the standard deviation of 4%

```{r}
#Generate alpha and beta for beta distributions priors for the three ads (A,B, and C)
a <- Beta_Parameters(0.15,0.0016)#0.04^2
b <- Beta_Parameters(0.16,0.0016)
c <- Beta_Parameters(0.17,0.0016)

#Plot prior beta distibutions for the three ads (A,B, and C)
x = seq(0, 0.4, length = 10000)
y = dbeta(x, a$alpha, a$beta)
y1 = dbeta(x, b$alpha, b$beta)
y2 = dbeta(x, c$alpha, c$beta)
{plot(x, y, type = "l", col = "blue", ylim = c(0, 12),xlim = c(0, .40),lwd = 2, ylab =' Density', xlab = expression(theta))
lines(x, y1, col = "orange", lwd = 2)
lines(x, y2, col = "black", lwd = 2)
legend(0.1, 12, legend = c("A", "B", "C"), col = c("blue","orange","black"), lwd = 5, cex = 1, horiz = TRUE)}
```

These 3 prior distributions for ad A, B and C are beta distributions (they look identical to bell curves assumptions you had made). For ad A, you had estimated α = 11.8 and β = 66.9 to produce the beta distribution with the assumption of a bell curve with the mean of 15% and sd of 4%. Similarly, ad B has  α = 13.3 and β = 69.7. Moreover, ad c has  α = 14.8 and β = 72.4. 

## Evaluate Data

With the help of Monte Carlo simulation it is found  ad C will outperform B in 57% cases. Similarly, C will do better than A for click rates in 64% cases. This prior information is not good enough for you to be conclusive about the performance of these ads. This is precisely the reason you why had launched this campaign/experiment with these ads being tested on a large number of recipients.

```{r}
#Simulate data to identify probability of ads A better than B with the prior distibutions
a_simulation = rbeta(10^6, a$alpha, a$beta)
b_simulation=rbeta(10^6, b$alpha, b$beta)
c_simulation=rbeta(10^6, c$alpha, c$beta)
mean(b_simulation > a_simulation)
mean(c_simulation > b_simulation)
mean(c_simulation > a_simulation)
```

## 1 Hour Data

The ad campaign has been running for just an hour and a few reults are availble:

| Ads | Total Emails Sent | Clicks | Opened |
| ---- | ---- | ---- | ---- |
| A | 5500 | 4 | 58 |
| B | 9000 | 14 | 101 |
| C | 12500 | 23 | 129 |

### Frequentist Approach

Ad A, there are not yet enough sample of clicks, just 4, to use them for A/B testing by the frequentist approach. 

Need to have at least 10 instances of both clicks and non-clicks. This requirement can be relaxed to 5 clicks/non-click with a few caveats but the sample size should never fall below 5. Notice that for ad-A you have just 4 clicks which are not a sufficient sample size. Hence, can only compare performance for ad-B against ad-C. 

```{r}
prop.test(x = c(14, 23), n = c(101, 129), correct = FALSE, alternative = "greater")
success <- c(14, 23)
n <- c(101, 129)
prop.test(success, n, correct = FALSE, alternative = "greater")

```

The most important part in this result is the p-value of 0.7919 ~ 0.79. Based on these results, ad-C is not significantly better than ad-B.

### Bayesian Approach

Bayesian A/B testing can  incorporate these samples to produce meaningful results - the posterior distribution (new knowledge).

```{r}
########### 1st hour ##############################
#Results from the campaign in the first hour 

a1=4
an1=58
b1=14
bn1=101
c1=23
cn1=129

#Plot posterior beta distibutions for the three ads (A,B, and C) afterincorporating evidence from the first hour
x=seq(0,0.4,length=10000)
y=dbeta(x,a$alpha+a1,a$beta+an1-a1)
y1=dbeta(x,b$alpha+b1,b$beta+bn1-b1)
y2=dbeta(x,c$alpha+c1,c$beta+cn1-c1)
plot(x,y, type="l", col="blue",ylim = c(0,18),xlim=c(0,.40),lwd=2,ylab='Density',xlab= expression(theta), main = "Distributions After 1 Hour")
lines(x,y1, col="orange",lwd=2)
lines(x,y2,col="black",lwd=2)
legend(0.1,18,legend = c("A", "B", "C"),col=c("blue","orange","black"), lwd=5, cex=1, horiz = TRUE)
main = "Distributions AFter 1 Hour"
```

B is performing better than A in 81% cases. Already, with the limited sample size, there is an improvement on our priors (old knowledge). C will outperform A in 94% cases. 

```{r}
#Simulate data to identify probability of ads A better than B with the prior distibutions
a_simulation=rbeta(10^6,a$alpha+a1,a$beta+an1-a1)
b_simulation=rbeta(10^6,b$alpha+b1,b$beta+bn1-b1)
c_simulation=rbeta(10^6,c$alpha+c1,c$beta+cn1-c1)
mean(b_simulation>a_simulation)
mean(c_simulation>b_simulation)
mean(c_simulation>a_simulation)
```

## 5 Day Data

After collecting data for five days all the possible results from the campaign are collected. These were the results from the previous part as well while doing A/B testing by the frequentist method.

| Ads | Total Emails Sent | Clicks | Opened |
| ---- | ---- | ---- | ---- |
| A | 5500 | 41 | 554 |
| B | 9000 | 98 | 922 |
| C | 12500 | 230 | 1235 |

Using this new and improved evidence, calculate the new beta distributions for the click rates for the 3 ads.

```{r}
######### Complete Experiment ##############
#The final results from the campaign after the 5th day
a1=41
an1=554
b1=98
bn1=922
c1=230
cn1=1235

#Plot posterior beta distibutions for the three ads (A,B, and C) afterincorporating the final evidence from the campaign
x=seq(0,0.4,length=10000)
y=dbeta(x,a$alpha+a1,a$beta+an1-a1)
y1=dbeta(x,b$alpha+b1,b$beta+bn1-b1)
y2=dbeta(x,c$alpha+c1,c$beta+cn1-c1)

#Simulate data to identify probability of ads A better than B with the prior distibutions
plot(x,y, type="l", col="blue",ylim = c(0,46),xlim=c(0,0.4),lwd=2,ylab='Density',xlab= expression(theta))
lines(x,y1, col="orange",lwd=2)
lines(x,y2,col="black",lwd=2)
legend(0.085,46.5,legend = c("A", "B", "C"),col=c("blue","orange","black"), lwd=5, cex=1, horiz = TRUE)
```

It is nearly certain ad C had outperformed both ad B and A in terms of click rates. Could create new sets of experiments with variants of ad C to further improve the performance on the click rates.

```{r}
a_simulation=rbeta(10^6,a$alpha+a1,a$beta+an1-a1)
b_simulation=rbeta(10^6,b$alpha+b1,b$beta+bn1-b1)
c_simulation=rbeta(10^6,c$alpha+c1,c$beta+cn1-c1)
mean(b_simulation>a_simulation)
mean(c_simulation>b_simulation)
mean(c_simulation>a_simulation)
```


# Appendix

## Thompson Sampling

William R. Thompson proposed a sampling method that exploits Bayesian priors in his paper published in 1933. In the paper, Thompson was trying to design an effective sampling strategy for clinical trials to save as many patients while exploring new drugs and methods of treatment. The loss of a patient to a trial drug despite the availability of better drug is called regret. A clinical trial is essentially an explore/exploit optimization where one is trying to minimize regret while finding new and effective ways to treat the patients.

Thompson pointed that when the sampling design exploits the Bayesian methods to constantly update sample distribution based on new knowledge the regret can be minimized. 

## Uneven Sampling

Recall the number of ads tested:

| Ads | Total Emails Sent | 
| ---- | ---- | 
| A | 5500 |
| B | 9000 | 
| C | 12500 | 

How is this optimal?

In the beginng, C was beleived to be the most likely to be successful followed by B then A.  How, this a beleif, not a fact.  It was not certain what the click rate for C would be compared to A.  Had the emails been divided into equal 1/3, the previous knowledge would not have been leveraged.  

The sample design included 46.3% recipients get C ad vs. 20.4% for A. Ad B was sent to the 33% recipients.  The interesting part is this is an evolutionary design. With each level of experiment and evidence, the sample design will get optimized to generate more profit. This design is highly flexible. New ads (say D, and E) can be added and while removing underperforming ads (e.g. ad A) with reasonable ease.
