---
title: "O365 Concierge Bias Analysis"
output:
  rmdformats::readthedown:
    highlight: pygments
---

<style type="text/css">
p{ /* Normal  */
   font-size: 12px;
}
body{ /* Normal  */
   font-size: 12px;
}
td {  /* Table  */
   font-size: 10px;
}
h1 { /* Header 1 */
 font-size: 26px;
 color: #4294ce;
}
h2 { /* Header 2 */
 font-size: 22px;
}
h3 { /* Header 3 */
 font-size: 18px;
}
code.r{ /* Code block */
  font-size: 10px;
}
pre { /* Code block */
  font-size: 10px
}
#table-of-contents h2 {
background-color: #4294ce;
}
#table-of-contents{
background: #688FAD;
}
#nav-top span.glyphicon{
color: #4294ce;
}
#postamble{
background: #4294ce;
border-top: ;
}
</style>

```{r myScripts, echo=FALSE}
Beta_Parameters <- function(mean, variance) {
  alpha <- ((1 - mean) / variance - 1 / mean) * mean ^ 2
  beta <- alpha * (1 / mean - 1)
  return(Beta_Parameters = list(alpha = alpha, beta = beta))
}
findOutliers <- function(dt, var) {
  var_name <- eval(substitute(var),eval(dt))
  tot <- sum(!is.na(var_name))
  na1 <- sum(is.na(var_name))
  m1 <- mean(var_name, na.rm = T)
  par(mfrow=c(2, 2), oma=c(0,0,3,0))
  boxplot(var_name, main="With outliers")
  hist(var_name, main="With outliers", xlab=NA, ylab=NA)
  outlier <- boxplot.stats(var_name)$out
  mo <- mean(outlier)
  var_name <- ifelse(var_name %in% outlier, NA, var_name)
  boxplot(var_name, main="Without outliers")
  hist(var_name, main="Without outliers", xlab=NA, ylab=NA)
  title("Outlier Check", outer=TRUE)
  na2 <- sum(is.na(var_name))
  message("Outliers identified: ", na2 - na1, " from ", tot, " observations")
  message("Proportion (%) of outliers: ", (na2 - na1) / tot*100)
  message("Mean of the outliers: ", mo)
  m2 <- mean(var_name, na.rm = T)
  message("Mean without removing outliers: ", m1)
  message("Mean if we remove outliers: ", m2)
  # response <- readline(prompt="Do you want to remove outliers and to replace with NA? [yes/no]: ")
  # if(response == "y" | response == "yes"){
  #   dt[as.character(substitute(var))] <- invisible(var_name)
  #   assign(as.character(as.list(match.call())$dt), dt, envir = .GlobalEnv)
  #   message("Outliers successfully removed", "\n")
  #   return(invisible(dt))
  # } else{
  #   message("Nothing changed", "\n")
  #   return(invisible(var_name))
  # }
}
# https://datascienceplus.com/identify-describe-plot-and-removing-the-outliers-from-the-dataset/
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("plyr", "dplyr", "ggplot2", "tidyr", "stringr", "lubridate", "ggpubr", "gridExtra", "readxl", "scales", "modes", prompt = FALSE)

options(scipen = 999)#Avoid exponents in plots
```
# Quick Facts

Below is a summary of the findings found inside this document.

- The log(TTR) histogram presents a bimoal distribution.  TTR data is not Gaussian (normal)
- TTR appears to have outliers.  Requires investigation/validation.

**TTR Bias Analysis**

|  Metric        | Wilcox | t-test | Comments |
|----------------|--------|--------|----------|
| TTR            |    0   |   0    |  adopt alt hypothesis |
| Not ReScenario |   0    |   0    |  adopt alt hypothesis |
| Agent Reopen   | >0.05  | >0.05  | no statistical bias |
| Repeat Caller  | >0.05  | >0.05  | no statistical bias |

## Proportional Tests Summary

|  Metric        | prop.test | Comments |
|----------------|--------|--------|----------|
| Repeat Calleer |  >0.05 | no statistical bias |
| Customer Reopen | >0.05 | no statistical bias |
| Agent Reopen | 0.002308 | adopt alt hypothesis |
| IsResolved | >0.05 | no statistical bias |

- No material differences in the data explain the bimodal distribution of the TTR data.  More data is required to understand this distribution.
- Interesting - the longer the TTR duration, the higher the percentage Concierge involvement.

# Introduction

## Problem Statement

The goal of this analysis is to determine if the Concierge Agents compared to Agents using TTR as the comparison metric.  Additional metrics were added including Agent Reopen, Repeat Caller and Not ReScenario.

> Assumption - Records with FirstRelease in the Role description uniquely identifies Concierge Agents.  (Confirmed by Dean.)

## Methodology

A statistical test will be performed to to compare the mean TTR between the Concierge and Agents.  (The mean of the Agent TTR is used as the theoretical mean $\mu$.) This is expressed as a hypothesis where the null hypothesis is *There is no difference in the TTR between Concierge and Agents*:

$H_0: m = \mu$  
where *m* is the mean TTR for Agents and $\mu$ is the mean TTR for Concierge.

If the p-value is less than or equal to the significance level 0.05, reject the null hypothesis and accept the alternative hypothesis. In other words, conclude the sample mean is significantly different from the theoretical mean.

# Get Data

Dean provided data on 12/13/17.

```{r}
myData <- read_excel("~/Github/Valorem/ClientProjects/MS_O365/data/CaseNotesSearchABHistorical_Dec13.xlsx")

myData2 <- myData %>% mutate(Date = as.Date(CreateDateTime), Hour = hour(CreateDateTime), ReScenario = factor(ReScenario)) %>%
  filter(TTR > 0) %>% drop_na(IsResolved, Roles) %>% mutate(Concierge = str_detect(Roles, "FirstRelease")) %>% 
  mutate(Agent = str_detect(Roles, "Agent")) %>%    
  select(-RequestID, -PartnerId,-CreateDateTime,  -Date, -Hour)
# remove 3 records where Agent = FALSE and Rave2 = FALSE:  table(myData2$Agent)
myData2 <- filter(myData2, Agent == TRUE)
myData2 <- myData2 %>% mutate(NewRole = ifelse(Concierge == TRUE, "Concierge", "Agent"))
glimpse(myData2)
table(myData2$NewRole)
```

The original data has `r nrow(myData)` records.  After removing records where `IsRevolved = 0` and `Roles = NA`, `r nrow(myData2)` records remain for analysis.

After modifying the data, `r table(myData2$Concierge)[2]` records of `r nrow(myData2)` have *FirstRelease* in the role description. `r percent(table(myData2$NewRole)[2]/nrow(myData2))` of the records were created by Concierge agents.

## Outliers

```{r}
out <- ggplot(myData2, aes(x = "", y = TTR)) + geom_boxplot(outlier.color="red", outlier.shape=8, outlier.size=4)
out
```

There are outliers in the data.  The outliers will influence the normality of the data.  The extreme values are **not** removed at this time.

```{r}
findOutliers(myData2, TTR)
```

## Data Transformations

Because TTR must be greater than 0, a histogram of TTR values is expected to be right skewed.  Indeed, that is what is shown above in the histograms with and without outliers.  This can be corrected by transforming the data to make it appear more normal.  The most common transformation is log(10):

```{r message=FALSE, warning=FALSE}
p1 <- ggplot(aes(x = TTR), data = myData2) + geom_histogram() + ggtitle("TTR")
p2 <- p1 + scale_x_log10() + ggtitle("log(TTR)")

grid.arrange(p1, p2, ncol=2)

pX <- ggplot(aes(x = log(TTR)), data = myData2) + geom_histogram() + ggtitle("TTR") + scale_x_log10()
```

The transformed TTR histogram does not result in a near-normal distribution.  This is a bimodal distribution where two processes are often represented in one data set.  For example, a distribution of production data from a two-shift operation might be bimodal if each shift produces a different distribution of results.  (This is explored in the Appendix.)

```{r eval=FALSE, echo=FALSE}
#Exercise to reduce bimodal to something closer to norm
#Ill-advised to use this transform - removes too much information
#No monotonic transformation can do what we want and a non-monotonic transformation is usually a bad idea. 
transformed <- abs(myData2 - mean(myData2))
shapiro.test(transformed)
hist(transformed)
```

# Statistical Analysis

In this analysis, a non-parametric test and a parametric t-test will be used to test the null hypothesis.

* Non-Parametric Tests:  A statistical method is called non-parametric if it makes no assumption on the population distribution or sample size. This is in contrast with most parametric methods in elementary statistics that assume the data is quantitative, the population has a normal distribution and the sample size is sufficiently large.  In general, conclusions drawn from non-parametric methods are not as powerful as the parametric ones. However, as non-parametric methods make fewer assumptions, they are more flexible, more robust, and applicable to non-quantitative data.

* Many statistical tests including correlation, regression, t-test, and analysis of variance (ANOVA) assume some certain characteristics about the data. They require the data to follow a normal distribution or Gaussian distribution. These tests are called parametric tests because their validity depends on the distribution of the data.  If the sample size is large enough (n > 30), the distribution of the data can be ignored and use parametric tests.  The central limit theorem states no matter what distribution things have, the sampling distribution tends to be normal if the sample is large enough (n > 30).

> Had the data been normal, density and Q-Q plots are useful visualizations to confirm normality.

## TTR Statistical Analysis

### Non-Parametric Test

There are a number of statistical methods that do not require normal data.  This analysis will perform a Wilcoxon Test.  

Two data samples are independent if they come from distinct populations and the samples do not affect each other. Using the Wilcoxon Signed-Rank Test, it can be determined of the population distributions are identical without assuming them to follow the normal distribution.

Without assuming the data to have normal distribution, decide at .05 significance level if the TTR data of Agent and Concierge roles in the data have identical data distribution.

```{r}
wilcox.test(TTR ~ NewRole, data = myData2)
```

The null hypothesis is TTR data of Agent and Concierge roles are identical populations. To test the hypothesis, apply `wilcox.test` to compare the independent samples. As the p-value turns out to be significantly less than the .05 significance level, therefore reject the null hypothesis.  The TTR values are statistically different between the Agent and Concierge roles.

### t-test

A large number of statistical tests are based on the assumption of normality, so not having data that is normally distributed typically creates uncertainty on statistical analysis. However, if the test you are running is not sensitive to normality, it is acceptable to still run parametric tests even if the data are not normal.

Several tests are *robust* to the assumption of normality, including t-tests (1-sample, 2-sample, and paired t-tests), Analysis of Variance (ANOVA), Regression, and Design of Experiments (DOE). The the non-normal data does not have very long tails and outliers, a t-test can be used for non-normal data.  [See this explanation](http://thestatsgeek.com/2013/09/28/the-t-test-and-robustness-to-non-normality/)

Outlier detection performed previously found outliers in the data.  Returning to this analysis, examine the outliers by Agent and Concierge roles.

```{r}
ggplot(myData2, aes(x=NewRole, y=TTR)) + geom_boxplot()
```

Both roles have severe outliers.  For the purposes of this t-test, remove the extreme values.  (Recall how boxplots work - the *body* of the boxplot corresponds to the second + third quartiles of the data (= interquartile range, IQR) and each whisker limit is calculated taking 1.5*IQR beyond the end of that body.  If you take the hypothesis that your data has a normal distribution, there are this amount of data outside each whisker:

$1-pnorm(qnorm(0.75)+1.5*2*qnorm(0.75)) = 0.0035$ Therefore, a normal variable has 0.7% of boxplot outliers.

```{r}
#Consider using outliers package
extremeOutliers <- boxplot(myData2$TTR)
str(extremeOutliers)
myData3 <- myData2[!(myData2$TTR %in% extremeOutliers$out),]
ggplot(myData3, aes(x=NewRole, y=TTR)) + geom_boxplot()
```

The data still has outliers but they are not as extreme.

### t-test Calculation

The one-sample t-test compares a sample’s mean with a known value, when the variance of the population is unknown. In this analysis the goal is to assess the Concierge TTR and compare it with the Agent TTR (a known value).

```{r}
myGrp <- group_by(myData3, NewRole)
myMeans <-  summarize(myGrp, Mean = mean(TTR))
myMeans
```

```{r}
t.test(TTR ~ NewRole, myData3)
```

The results from this test suggest there is a statistically valid difference between the TTR means between Agents and Concierge at a 95% confidence level.

While it is known the data is not normal, perform a log transformation anyway to determine if a similar finding is found.

```{r}
t.test(log(TTR) ~ NewRole, myData3)
#wilcox.test(TTR ~ NewRole, data = myData3)
```

Same result.  The p-value is significantly less than 0.05 so the null hypothesis is rejected and the difference in the means is statistically significant.

### Side Bar - Compare Variances

The t-test assumes the variances between TTR for Agent and Concierge roles are teh same, or nearly so.  It is often useful to testing this assumption. This can be done quickly and easily using `var.test`

```{r}
var.test(TTR~NewRole, myData3)
```

While it appears there is trong evidence that teh bariances are not equal, it is not a valid test for our data.

> var.test is not robust against departures from a normal distribution.

## Customer Reopen Statistical Analysis

Because the process used for statistical evaluation were detail above for TTR, the tests for *Customer ReOpen* will focus on the test and the results only.

The only difference is the data must be further subset.  In this case, records where `ReScenario = Customer Reopen` must be used - the rest of the records will be excluded from the analysis.  Filtering `Cusotmer ReOpen` results in `r table(myData2$ReScenario)[2]` records.

```{r}
table(myData2$ReScenario)
myData2_CustReopen <- filter(myData2, ReScenario == "Customer Reopen")
```

### Non-Parametric Test

```{r}
wilcox.test(TTR ~ NewRole, data = myData2_CustReopen)
```

### t-test Calculation

```{r}
t.test(TTR ~ NewRole, myData2_CustReopen)

t.test(log(TTR) ~ NewRole, myData2_CustReopen)
```

**Conclusion**:  The Customer Reopen ReScenario value is not significantly different between the Agent and Concierge roles.

## Customer Not Rescenario Statistical Analysis

```{r}
table(myData2$ReScenario)
myData2_NotReScenario <- filter(myData2, ReScenario == "Not ReScenario")
```

### Non-Parametric Test

```{r}
wilcox.test(TTR ~ NewRole, data = myData2_NotReScenario)
```

### t-test Calculation

```{r}
t.test(TTR ~ NewRole, myData2_NotReScenario)

t.test(log(TTR) ~ NewRole, myData2_NotReScenario)
```

## Agent Reopen Statistical Analysis

```{r}
table(myData2$ReScenario)
myData2_AgentReopen <- filter(myData2, ReScenario == "Agent Reopen")
```

### Non-Parametric Test

```{r}
wilcox.test(TTR ~ NewRole, data = myData2_AgentReopen)
```

### t-test Calculation

```{r}
t.test(TTR ~ NewRole, myData2_AgentReopen)

t.test(log(TTR) ~ NewRole, myData2_AgentReopen)
```

## Repeat Caller Statistical Analysis

```{r}
table(myData2$ReScenario)
myData2_RepeatCaller <- filter(myData2, ReScenario == "Repeat Caller")
```

### Non-Parametric Test

```{r}
wilcox.test(TTR ~ NewRole, data = myData2_RepeatCaller)
```

### t-test Calculation

```{r}
t.test(TTR ~ NewRole, myData2_RepeatCaller)

t.test(log(TTR) ~ NewRole, myData2_RepeatCaller)
```

## Statistical Analysis Sumarry

> Very low values represented as 0 below.

|  Metric        | Wilcox | t-test | Comments |
|----------------|--------|--------|----------|
| TTR            |    0   |   0    |  adopt alt hypothesis |
| Not ReScenario |   0    |   0    |  adopt alt hypothesis |
| Agent Reopen   | >0.05  | >0.05  | no statistical bias |
| Repeat Caller  | >0.05  | >0.05  | no statistical bias |

# Conclusions

1. The mean `TTR` between Agents and Concierge are statistically different at a 95% confidence level.
2. The mean `TTR` between Agents and Concierge in `Not ReScenario` are statistically different at a 95% confidence level.
3. The mean `TTR` between Agents and Concierge in `Agent Reopen` are **NOT** statistically different at a 95% confidence level.
4. The mean `TTR` between Agents and Concierge in `Repeat Caller` are **NOT** statistically different at a 95% confidence level.

# Proportion Tests

To perform a proportion test, a bit of data manipulation is needed.  The arguments to `prop.test` must be given as 2 vectors where the first contains the number of positve outcomes and the other vector is the total numbers for each group (Agents and Concierge).

## Repeat Caller

```{r proportionRepeat}

v1_1 <- nrow(filter(myData2, ReScenario == 'Repeat Caller' & NewRole == "Agent"))
v1_2 <- nrow(filter(myData2, ReScenario == 'Repeat Caller' & NewRole == "Concierge"))
myVectorPositive <- c(v1_1, v1_2)
v2_1 <- nrow(filter(myData2, NewRole == "Agent"))
v2_2<- nrow(filter(myData2, NewRole == "Concierge"))
myVectorTotal <- c(v2_1, v2_2)

prop.test(myVectorPositive, myVectorTotal, correct =  F)
```

##Customer Reopen

```{r proportionCustomerReopen}
v1_1 <- nrow(filter(myData2, ReScenario == 'Customer Reopen' & NewRole == "Agent"))
v1_2 <- nrow(filter(myData2, ReScenario == 'Customer Reopen' & NewRole == "Concierge"))
myVectorPositive <- c(v1_1, v1_2)
v2_1 <- nrow(filter(myData2, NewRole == "Agent"))
v2_2<- nrow(filter(myData2, NewRole == "Concierge"))
myVectorTotal <- c(v2_1, v2_2)

prop.test(myVectorPositive, myVectorTotal, correct =  F)
```

## Agent Reopen

```{r proportionAgentReopen}
v1_1 <- nrow(filter(myData2, ReScenario == 'Agent Reopen' & NewRole == "Agent"))
v1_2 <- nrow(filter(myData2, ReScenario == 'Agent Reopen' & NewRole == "Concierge"))
myVectorPositive <- c(v1_1, v1_2)
v2_1 <- nrow(filter(myData2, NewRole == "Agent"))
v2_2<- nrow(filter(myData2, NewRole == "Concierge"))
myVectorTotal <- c(v2_1, v2_2)

prop.test(myVectorPositive, myVectorTotal, correct =  F)
```

## IsResolved

```{r proportionIsResolved}
table(myData2$IsResolved)
v1_1 <- nrow(filter(myData2, IsResolved == TRUE & NewRole == "Agent"))
v1_2 <- nrow(filter(myData2, IsResolved == TRUE & NewRole == "Concierge"))
myVectorPositive <- c(v1_1, v1_2)
v2_1 <- nrow(filter(myData2, NewRole == "Agent"))
v2_2<- nrow(filter(myData2, NewRole == "Concierge"))
myVectorTotal <- c(v2_1, v2_2)

prop.test(myVectorPositive, myVectorTotal, correct =  F)
```

## Proportional Tests Summary

|  Metric        | prop.test | Comments |
|----------------|--------|--------|----------|
| Repeat Calleer |  >0.05 | no statistical bias |
| Customer Reopen | >0.05 | no statistical bias |
| Agent Reopen | 0.002308 | adopt alt hypothesis |
| IsResolved | >0.05 | no statistical bias |

# Appendix (WIP)

```{r}
rm(list= ls()[!(ls() %in% c('myData','myData2', 'p2'))])
```

## Bimodel Distribution (WIP)

Because the TTR distribution was a surprise because of the bimodal shape, additional data exploration is performed to see if this unusual distribution exits in other areas and if it is common across roles.

The team also asked to determine in the bimodal nature could be explained by:

- Agents are encourged to close cases in 24 hours and 1 week
- Examine the 2nd peak - does it look like it might be escalated?  (Probably do not have the data to determine this.)

Recall the initial distribution:

```{r message=FALSE}
p2
p2Data <- ggplot_build(p2)
#Get values actually plotted you can use function ggplot_build() where argument is your plot.

# This will make list and one of sublists is named data. This sublist contains dataframe with values used in plot, for example, for histrogramm it contains y values (the same as count). If you use facets then column PANEL shows in which facet values are used. If there are more than one geom_ in your plot then data will contains dataframes for each
#If you need just data it seems layer_data is designed precisely for this: layer_data(p2,1)
layer_data(p2,1)
```

First, it appears to be a bimodal distribution, but are there more than 2 pearks?  What does the data suggest?

It is sometimes easier to view the hostogram as a density plot.

```{r}
plot(density(log(myData2$TTR)))
```

Above, there appears to be evidence of more thna 2 peaks.  Examine this.  First use the helpful `modes` package to collect quantitative data.

```{r}
#modes package https://cran.r-project.org/web/packages/modes/modes.pdf
modeData <- log(myData2$TTR)
amps(modeData)
modes(modeData, type=1, nmore=2)
#nth_highest(modeData, k = 1)
```

Quantitatively, there appear to be 12 peaks.  Apply judgement to the data using clustering to challenge this conclusion.

```{r}
#Kmeans
df <- data.frame(log(myData2$TTR))
colnames(df) = "X"
km <- kmeans(df, centers=2)
df$clust <- as.factor(km$cluster)

km2 <- ggplot(df, aes(x=X)) + geom_histogram(aes(fill=clust,y=..count../sum(..count..)),
                     binwidth=0.5, color="grey50") + stat_density(geom="line", color="red")

km <- kmeans(df, centers=3)
df$clust <- as.factor(km$cluster)

km3 <- ggplot(df, aes(x=X)) + geom_histogram(aes(fill=clust,y=..count../sum(..count..)),
                     binwidth=0.5, color="grey50") + stat_density(geom="line", color="red")

km <- kmeans(df, centers=4)
df$clust <- as.factor(km$cluster)

km4 <- ggplot(df, aes(x=X)) + geom_histogram(aes(fill=clust,y=..count../sum(..count..)),
                     binwidth=0.5, color="grey50") + stat_density(geom="line", color="red")

km <- kmeans(df, centers=5)
df$clust <- as.factor(km$cluster)

km5 <- ggplot(df, aes(x=X)) + geom_histogram(aes(fill=clust,y=..count../sum(..count..)),
                     binwidth=0.5, color="grey50") + stat_density(geom="line", color="red")

grid.arrange(km2, km3, km4, km5, ncol=2)
```

Logically, the first plot makes the most logical sense - this confirms our initial conclusion there are 2 peaks.

Referring to the histogram, the first peak is y = 21108 and the 2nd peak appears y = 11590.  The x values for these maximums are 1.4066512 and 3.1649652, respectively.  The antilogs are `r 10^1.4066512` and `r 10^3.1649652`.  Recalling the 1 day = 1440 minutes, the peaks occurs approximately at 25 minutes and 1 day.

```{r}
10^1.4066512
10^3.1649652
```

It is also useful to note the minima just before the 2nd peak occurs where y = 246 and x = 2.8133024 (`r 10^2.8133024` minutes).  To isolate the values for the 2nd peak, examine the values of x > 650.

```{r}
myData2Peak <- myData2 %>% filter(TTR > 650)
myData2Min <- myData2 %>% filter(TTR < 651)

myTablePeak <- table(myData2Peak$NewRole, myData2Peak$ReScenario)
myTableMin <- table(myData2Min$NewRole, myData2Min$ReScenario)

myPropTablePeak <- data.frame(prop.table(myTablePeak,1))
myPropTableMin <- data.frame(prop.table(myTableMin,1))

myPropTablePeak <- myPropTablePeak %>% mutate(Peak = "Peak2")
myPropTableMin <- myPropTableMin %>% mutate(Peak = "Peak1")

myPropTableAll <- rbind(myPropTableMin, myPropTablePeak)

myPropTableAll <- plyr::rename(myPropTableAll, c('Var1' = 'AgentType', 'Var2' = 'ReScenario'))

p1 <- ggplot(myPropTableAll, aes(x = AgentType, y = Freq, fill=ReScenario)) + geom_bar(stat = 'identity') + facet_grid(~Peak)
p1

target <- c("Agent Reopen", "Customer Reopen", "Repeat Caller")
myPropTableAll2 <- myPropTableAll %>% filter(ReScenario %in% target)

ggplot(myPropTableAll2, aes(x = AgentType, y = Freq, fill=ReScenario)) + geom_bar(stat = 'identity', position = "fill") + facet_grid(~Peak)
```

### Conclusion

Did not find material differences between the peaks in the bimodal TTR distribution within the available data.

## Understand Extreme TTR Values

Earlier in the analysis, we learned there are extreme TTR values.  Evalaute them to determin if these records share simliar characteristics.

Assume an extreme value is defined as any TTR > 2 weeks:

```{r}
myOutliers <- myData2 %>% filter(TTR > 14*1440)
nrow(myOutliers)
hist(myOutliers$TTR)

table(myOutliers$NewRole)


myDFtmp <- data.frame(matrix(ncol=3, nrow=0))
colnames(myDFtmp) <- c("Day", "Count", "PCT_Concierge")
                      
for(i in 1:50){
  tmpRecords <- myData2 %>% filter(TTR > (i-1)*1440 & TTR < (1+i)*1440)
  myDFtmp[i,1] <- i
  myDFtmp[i,2] <- nrow(tmpRecords)
  myDFtmp[i,3] <- round(((table(tmpRecords$NewRole)[2])/nrow(tmpRecords))*100,2)
  
}
#Change NAs to 0
myDFtmp[is.na(myDFtmp$PCT_Concierge),] <- 0

pp1 <- ggplot(myDFtmp, aes(x=Day, y = Count)) + geom_line()
pp2 <- ggplot(myDFtmp, aes(x=Day, y = PCT_Concierge)) + geom_line() +
  geom_smooth()
```



## Agent EDA

```{r}
myData_Agent <- myData %>% mutate(Date = as.Date(CreateDateTime), Hour = hour(CreateDateTime), ReScenario = factor(ReScenario)) %>%
  filter(TTR > 0) %>% mutate(Concierge = str_detect(Roles, "FirstRelease")) %>% mutate(Agent = str_detect(Roles, "Agent")) 

myData_Agent <- myData_Agent %>% mutate(NewRole = ifelse(Concierge == TRUE, "Concierge", "Agent"))
myData_Agent <- myData_Agent %>% na.omit(NewRole)
glimpse(myData_Agent)

myGrp_PartnerID <- group_by(myData_Agent, PartnerId, NewRole)
myAgents <- summarize(myGrp_PartnerID, n())
myAgents <- plyr::rename(myAgents, c("n()" = "count"))#plyr

ggplot(myAgents, aes(x = PartnerId, y = count, fill = NewRole)) + geom_bar(stat = "identity")
```

## To Do

2. Email Survey analysis

