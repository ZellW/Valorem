---
title: "Microsoft O365 A?B Testing Statistics"
output:
  rmdformats::readthedown:
    highlight: pygments
---

<style type="text/css">
p{ /* Normal  */
   font-size: 12px;
}
body{ /* Normal  */
   font-size: 12px;
}
td {  /* Table  */
   font-size: 10px;
}
h1 { /* Header 1 */
 font-size: 26px;
 color: #4294ce;
}
h2 { /* Header 2 */
 font-size: 22px;
}
h3 { /* Header 3 */
 font-size: 18px;
}
code.r{ /* Code block */
  font-size: 10px;
}
pre { /* Code block */
  font-size: 10px
}
#table-of-contents h2 {
background-color: #4294ce;
}
#table-of-contents{
background: #688FAD;
}
#nav-top span.glyphicon{
color: #4294ce;
}
#postamble{
background: #4294ce;
border-top: ;
}
</style>

```{r echo=FALSE, warning=FALSE, message=FALSE}
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("dplyr", "ggplot2","pwr", "bayesAB", "janitor", "lubridate", "hms", "stringr", "readr", "forcats",
        "RcppRoll", "tibble", "gridExtra", prompt = FALSE)
```

# Sample Size Calculation

## Introduction

Running an A/B test without thinking about statistical confidence is worse than not running a test at all — it gives you false confidence that you know what works for your site, when the truth is that you don’t know any better than if you hadn’t run the test.

There are a few tenents that drive sample size for a meaningful A/B test:

1. We want to be reasonably sure that we don’t have a false positive—that there is no real difference, but we calculate one anyway. This a Type I error.
2. We want to be reasonably sure that we don’t miss a positive outcome (or get a false negative). This is called Type II error.
3. We want to know whether a variation is better, worse or the same as the original. Why know the difference between worse vs same? I probably won’t switch from the original if the variation performs worse, but I might still switch even if it’s the same — for a design or aesthetic preference, for example.

What not to do:

1. Don’t continuously test for significance as your sample grows, or blindly keep the test running until you reach statistical significance. 

*If you stop your test as soon as you see “significant” differences, you might not have actually achieved the outcome you think you have. As a simple example of this, imagine you have two coins and you think they might be weighted. If you flip each coin 10 times, you might get heads on one all of the time, and tails on the other all of the time. If you run a statistical test comparing the portion of flips that got you heads between the two coins after these 10 flips, you’ll get what looks like a statistically significant result—if you stop now, you’ll think they’re weighted heavily in different directions. If you keep going and flip each coin another 100 times, you might now see that they are in fact balanced coins and there is no statistically significant difference in the number of heads or tails.*

Also, if you run your test forever, you’ll eventually reach a large enough sample size that a 0.00001% difference results as significant. 

2. Don’t rely on a rule of thumb like “16 times your standard deviation squared divided by your sensitivity squared”. Same thing with the charts you see on some websites that don’t make their assumptions clear. It’s better than a rule of thumb like “100 conversions”, but the math isn’t so hard it’s worth skipping over, and you’ll gain an understanding of what’s driving required sample size in the process.

# Calculate Sample Size

1. Specify the outcome you’re trying to measure. Conversion ratres are typically chosed as the target measure but depending on what you’re testing, it might be button clicks, newsletter signups, etc. In almost every case, you’ll be measuring a proportion—e.g., the portion of landing page visitors who complete signup, or the portion of landing page visitors who sign up for a newsletter.
2. Decide how substantial of a difference you’d like to detect – this is the sensitivity of the test. Generally target an A/B test that will have a statistically meaningful sample size that detects a 10% difference in conversion rate (e.g., to detect 11% vs. 10% conversion rate). This is a somewhat arbitrary decision you’ll have to make — testing a reasonably large difference will help to make sure you don’t spend forever testing in a local minima, but instead that you are moving on to test potentially bigger changes. 
3. Calculate the required sample size based on your baseline conversion rate and your desired sensitivity. Since we’re dealing with proportions, we want to perform a simple statistical analysis called a *power analysis for two independent proportions*. 

Let’s break this down:

- **power analysis** is a statistical tool to determine the minimum sample size required so that you can be reasonably confident that you are detecting meaningful differences between two values.
- **two independent** since we fully separate visitors (they see only the A or only the B variant), our test is nominally independent; the results for variation A aren’t based on the results for variation B.
- **proportions** we’re comparing conversion rates, which are a proportion.

The function in R is `power.prop.test`:

```{r eval=FALSE}
power.prop.test(n = NULL, p1 = NULL, p2 = NULL, sig.level = 0.05,
                     power = NULL, alternative = c("two.sided", "one.sided"),
                     strict = FALSE)
```

Leave `n` null since that is what we are solving for. `p1` and `p2` are set based on our baseline conversion level (10% in our example) and the sensitivity we’re trying to detect (a 10% difference vs. baseline conversion, or 11% in our example). We want a two-sided alternative, because we’re interested in testing whether the variation is either higher or lower than the original.

`sig.level` (significance level) and `power` are a little bit more complicated to explain, but briefly:

- **Significance** governs the chance of a false positive. A significance level of 0.05 means that there is a 5% chance of a false positive. 
    - Answers the question: *What percentage of the time are we willing to be fooled into seeing an effect by random chance? *
- **Power** represents the probability of getting a false negative. A power of 0.80 means that there is an 80% chance that if there was an effect, we would detect it (or a 20% chance that we’d miss the effect). Most researchers assess the power of their tests using 0.80 for adequacy.
    - Anwsers the question. *What percentage of the time are we willing to miss a real effect

Typical statistical standards for these quantities are 80% for power (i.e., 20% chance of a false negative) and 5% for significance level. If we choose standards that are too strict, perhaps 95% for power and 1% for significance level, all our A/B tests will need to run longer and we will have to invest more time and resources into testing. We won’t be able to iterate quickly to solve our business problems. What if we relaxed these statistical standards? Then we risk making change after change in our product that does not improve anything and investing work from our developers and other team members in changes that do not move us forward toward our goals.

The effect of picking a significance level of 0.05 and power of 0.8 means that we are 4 times more likely to get a false negative than a false positive. We’re generally more concerned about getting a false positive—making a change that doesn’t actually improve things than we are about not making a change at all, which is why we accept a greater likelihood of a false negative.

When we plug these values in to R, we get results like:

```{r}
power.prop.test(p1 = 0.1, p2 = 0.11, power = 0.8, alternative = 'two.sided', sig.level = 0.05)
```

This means that we need about 15k observations for each variation to be confident that the two conversion rates are significantly different. For a test with just a variation and an original, this means we need about 30k observations in total. 

At the end of your test, if you’ve reached your pre-determined sample size and see a difference greater than your minimum sensitivity, you should have a statistically significant result. 

Finally, don’t be discouraged by the sample sizes required – in almost every case, they’re bigger than you’d like them be. If you’re fortunate enough to have a high traffic website, you can test a new variation every few days, but otherwise, you may need to run your tests for several weeks. It’s still much better to be testing something slowly than to test nothing at all.

## Example 7

The following code shows you how to test the difference between two rates in R, e.g., click-through rates or conversion rates. You can apply the code to your own data by replacing the URL to the example data with your file path. To test the difference between two proportions, you can use the function prop.test which is equivalent to Pearson’s chi-squared test. For small samples you should use Fisher’s exact test instead. Prop.test returns a p-value and a confidence interval for the difference between the two rates. The interpretation of a 95% confidence interval is as follows: When conducting such an analysis many times, then 95% of the displayed confidence intervals would contain the true difference. 

```{r}
library(readr)
# Specify file path: 
dataPath <- "https://www.inwt-statistics.de/files/INWT/downloads/exampleDataABtest.csv" 
# Read data 
data <- read_csv(file = dataPath)  
# Inspect structure of the data 
str(data)  
# Change the column names 
names(data) <- c("group", "time", "clickedTrue")  
# Change type of group to factor  
data$group <- as.factor(data$group)  
# Change type of click through variable to factor 
data$clickedTrue <- as.factor(data$clickedTrue)  
levels(data$clickedTrue) <- c("0", "1") 
# Compute frequencies and conduct test for proportions  
# (Frequency table with successes in the first column) 
freqTable <- table(data$group, data$clickedTrue)[, c(2,1)]  
# print frequency table 
freqTable
# Conduct significance test 
prop.test(freqTable, conf.level = .95) 
```

There are some more pitfalls, but most of them can easily be avoided. 

- First, as a counterpart of stopping your test early because of a significant result, you could gather more data after the planned end of the test because the results have not yet become significant. This would likewise lead to an α inflation. 
- A second, similar problem arises when running several tests at once: The probability to achieve a false-positive result would then be α for each of the tests. The overall probability that at least one of the results is false-positive is much larger. So always keep in mind that some of the significant results may have been caused by chance. 
- Third, you can also get into trouble when you reach the required sample size very fast and stop the test after a few hours already. You should always consider that the behavior of the users in this specific time slot might not be representative for the general case. To avoid this, you should plan the duration of the test so that it covers at least 24 hours or a week when customers are behaving different at the weekend than on a typical work day. 
- A fourth caveat concerns a rather moral issue: When users discover they are part of an experiment and suffer from disadvantages as a result, they might rightly become angry. (This problem will probably not arise due to a different-colored button, but maybe because of different prices or special offers.)

# Bayesian Approach

Read this:  https://econsultancy.com/blog/65755-using-data-science-with-a-b-tests-bayesian-analysis

Most A/B test approaches are centered around frequentist hypothesis tests used to come up with a point estimate (probability of rejecting the null) of a hard-to-interpret value. Oftentimes, the statistician or data scientist laying down the groundwork for the A/B test will have to do a power test to determine sample size and then interface with a Product Manager or Marketing Exec in order to relay the results. This quickly gets messy in terms of interpretability. More importantly it is simply not as robust as A/B testing given informative priors and the ability to inspect an entire distribution over a parameter, not just a point estimate.

Enter Bayesian A/B testing.

Bayesian methods provide several benefits over frequentist methods in the context of A/B tests - namely in interpretability. Instead of p-values you get direct probabilities on whether A is better than B (and by how much). Instead of point estimates your posterior distributions are parametrized random variables which can be summarized any number of ways. Bayesian tests are also immune to ‘peeking’ and are thus valid whenever a test is stopped.

Unlike a frequentist method, in a Bayesian approach you first encapsulate your prior beliefs mathematically. This involves choosing a distribution over which you believe your parameter might lie. As you expose groups to different tests, you collect the data and combine it with the prior to get the posterior distribution over the parameter(s) in question. Mathematically, you are looking for P(parameter | data) which is a combination of the prior and posterior (the math, while relatively straightforward, is outside of the scope of this brief intro).

As mentioned above, there are several reasons to prefer Bayesian methods for A/B testing (and other forms of statistical analysis!). First of all, interpretability is everything. Would you rather say “P(A > B) is 10%”, or “Assuming the null hypothesis that A and B are equal is true, the probability that we would see a result this extreme in A vs B is equal to 3%”? I think I know my answer. Furthermore, since we get a probability distribution over the parameters of the distributions of A and B, we can say something such as “There is a 74.2% chance that A’s λ is between 3.7 and 5.9.” directly from the methods themselves.

Secondly, by using an informative prior we alleviate many common issues in regular A/B testing. For example, repeated testing is an issue in A/B tests. This is when you repeatedly calculate the hypothesis test results as the data comes in. In a perfect world, if you were trying to run a Frequentist hypothesis test in the most correct manner, you would use a power test calculation to determine sample size and then not peek at your data until you hit the amount of data required. Each time you run a hypothesis test calculation, you incur a probability of false positive. Doing this repeatedly makes the possibility of any single one of those ‘peeks’ being a false positive extremely likely. An informative prior, means that your posterior distribution should make sense any time you wish to look at it. If you ever look at the posterior distribution and think “this doesn’t look right!”, then you probably weren’t being fair with yourself and the problem when choosing priors.

Furthermore, an informative prior will help with the low base-rate problem (when the probability of a success or observation is very low). By indicating this in your priors, your posterior distribution will be far more stable right from the onset.

What is a `prior`?

Simply put, a prior lets you specify some sort of, ahem, prior information about a certain parameter so that the end posterior on that parameter encapsualtes both the data you saw and the prior you inputted. Priors can come from a variety of places including past experiments, literature, and domain expertise into the problem. See this blogpost for a great example of somebody combining their own past data and literature to form very strong priors.

Priors can be weak or strong. The weakest prior will be completely objective and thus assign an equal probability to each value for the parameter. Examples of this include a Beta(1, 1) prior for the Bernoulli distribution. In these cases, the posterior distribution is completely reliant on the data. A strong prior will convey a very precise belief as to where a parameter’s values may lie. For example:

```{r}
plotBeta(1000, 1000)
```

The stronger the prior the more say it has in the posterior distribution. Of course, according to the Bernstein–von Mises theorem the posterior is effectively independent of the prior once a large enough sample size has been reached for the data. How quickly this is the case, depends on the strength of your prior.

Do you need (weak/strong) priors? Not necessarily. You can still leverage the interpretability benefits of Bayesian AB testing even without priors. At worst, you’ll also get slightly more pertinent results since you can parametrize your metrics as the appropriate distribution random variable. However, without priors of some kind (and to be clear, not random bullshit priors either) you run into similar issues as with Frequentist AB testing, namely Type 1 and Type 2 errors. A Type 1 error is calling one version better when it really isn’t, and a Type 2 error is calling a better version equal or worse. Both typically arise from low sample size/base rate and are controlled by reaching appropriate sample size as per a power calculation.

## Example 1

Let’s say we are testing two versions of Page 1, to see the CTR onto Page 2. For this example, we’ll just simulate some data with the properties we desire.

```{r}
A_binom <- rbinom(250, 1, .25)
B_binom <- rbinom(250, 1, .2)
```

Of course, we can see the probabilities we chose for the example, but let’s say our prior knowledge tells us that the parameter p in the Bernoulli distribution should roughly fall over the .2-.3 range. Let’s also say that we’re very sure about this prior range and so we want to choose a pretty strict prior. The conjugate prior for the Bernoulli distribution is the Beta distribution.

```{r}
plotBeta(100, 200) # looks a bit off
```

```{r}
plotBeta(65, 200) # perfect
```

Now that we’ve settled on a prior, let’s fit our bayesTest object.

```{r}
AB1 <- bayesTest(A_binom, B_binom, priors = c('alpha' = 65, 'beta' = 200), n_samples = 1e5, distribution = 'bernoulli')
```

`bayesTest` objects come coupled with `print`, `plot` and `summary` generics. Let’s check them out:

```{r}
print(AB1)
```

```{r}
summary(AB1)
```

```{r}
plot(AB1)
```

`print` talks about the inputs to the test, `summary` will do a P((A - B) / B > percentLift) and credible interval on (A - B) / B calculation, and `plot` will plot the priors, posteriors, and the Monte Carlo ‘integrated’ samples.

## Example 2

see https://rpubs.com/mbounthavong/301039

The setup:  Beta probability density function will have:

- alpha = 10
- beta = 10
- The control will have prior distribution of 50%
- The treatment will have a prior distribution of 30%

We are evaluating the hypothesis that the treatment group is has a lower probability of having an event occur compared to the control (30% versus 50%, respectively). Therefore, the control group can have a prior that follows a Bernoulli distribution.

First collection – Start with 20 observations each

In the first collection, we assume that the control follows a prior Bernoulli distribution of 50%. Imagine that this is the probablity of having an event. Therefore, a 50% probability of having an event is like a coin flip. However, with treatment, the probability of having an event is lower, 30%. We start with a sample size of 20 for each group.

```{r}
control_1 <- rbinom(20, 1, 0.5)
treatment_1 <- rbinom(20, 1, 0.3)
```

### First Analysis

In the first analysis, the distance and size of the treatment and control are slightly different. However, the probabilty that the treatment is better than control is 91.3%.

```{r}
test1 <- bayesTest(treatment_1, control_1, distribution = "bernoulli", priors = c("alpha" = 10, "beta" = 10))
print(test1)
```

```{r}
summary(test1)
```

```{r}
plot(test1)
```

20 additional observations are added to increase the sample size to 40. The expectation is that the more data that are available, the better the precision of the Bayesian posterior distribution. Again, maintaining that they control follows a prior Bernoulli distribution of 50% and the treatment follows a prior distribution of 30%, we obtain the following:

```{r}
control_2 <- rbind(control_1, rbinom(20, 1, 0.5))
treatment_2 <- rbind(treatment_1, rbinom(20, 1, 0.3))
```

### Second Analysis

The distance and size between the two distributions have increased. The probability that treatment is better than the control is 96.9%.

```{r}
test2 <- bayesTest(treatment_2, control_2, distribution = "bernoulli", priors = c("alpha" = 10, "beta" = 10))
print(test2)
```

```{r}
summary(test2)
```

```{r}
plot(test2)
```

## Example 3 - Exploratory IO R Version
https://blog.exploratory.io/an-introduction-to-bayesian-a-b-testing-in-exploratory-cb5a7ad80963

```{r}
myData <- read.csv("C:\\Exploratory\\data\\ab_testing.csv", stringsAsFactors = FALSE)
head(myData)
myData$date <- as.POSIXct(myData$date)
myData <- dplyr::rename(myData, landingPage = landingPagePath)

myData <- myData %>% mutate(conversion_rate = round(signUpCount/uniquePageView, 3), 
                            landingPage = recode(landingPage, `/post?id=11` = "B", `/post?id=12` = "A"))
head(myData)
```

There are several ways to evaluate the result with some techniques often used in Statistics. Traditionally, the most popular one is to use something called Chi-Squared Test.

## Chi-Squared Test

Let’s say we are testing two versions of our landing page (A and B in the data above) and monitoring how much ‘sign ups’ each of the pages is bringing in every day.
 
Note that this test doesn’t really care about the trend by the date. It’s about how much of the data we have. In reality, we need to first estimate how much data we need to collect before we run this test, but for a sake of simplifying this, we are assuming that we have collected just enough data already. (See the previous content on calculating the sample size.)

Also, note the ‘not_sign_up’ column as total - sign_up because Chi-Square Test wants to see the proportion of the data.

Now we can run the chi-square test.

```{r}
myData2 <- myData %>% arrange(landingPage)
myData2 <- myData2 %>% mutate(CR = if_else(landingPage  == "A",conversion_rate*1.3,conversion_rate))
myData2 <- myData2 %>%  group_by(landingPage) %>%  summarize(uniquePageView_sum = sum(uniquePageView, na.rm = TRUE), 
                                  signedUpCount_sum = sum(signUpCount, na.rm = TRUE))
myData2 <- myData2 %>% mutate(not_signed_up = uniquePageView_sum - signedUpCount_sum)
chisq.test(as.matrix(myData2[, 3:4]))
```
 
The most important thing we want to look at here is ‘p.value’ column. This shows the rate that this difference between A and B can happen by a random chance. In this case, the number is 0.2014, which means that this difference can happen by chance at a rate of 20%. And this means, in statistic world where this value needs to be less than 5% to state a statistical significance, we can’t conclude that the difference between A and B is statistically significant.

Maybe you might ask as a business person, what if we continue the testing and collect more data, then we’ll evaluate the result again? We might see the P-value going down to less than 5% at some point? The answer is No. Unfortunately, the test is already done. The reason we have run the test is that we have collected enough data for the test to be valid. That means, more data is not going to help the test. The only thing we can do is to re-design the test by starting from scratch. Ouch!

Along with this problem, there are a few challenges with using Chi-Square Test for A/B Testing especially for the modern organizations who need to make decisions quickly and iteratively.

- We need to know how much of the data we need to collect for the test before starting the test.
- We can’t test the result in real-time until we collect a full of the planned data size.
- The test result is not intuitively understandable especially for those without a statistical background.
- The test result is black and white, either it is statistically significant or not. Therefore, it’s hard to figure out what to do especially when it is not statistically significant.

## Bayesian A/B Test

There is another way to evaluate the test result and this actually fits the modern business world much better. It is called **Bayesian A/B Testing** which employs Bayesian inference methods to give you ‘probability’ of how much A is better (or worse) than B.

There are some important benefits of Bayesian A/B Testing: 

- The immediate advantage of this method is we can understand the result intuitively.
- You don’t have to worry too much about the test size when you evaluate the result. You can start evaluating the result from the first day of the test because when you run the test what you would get is the probability of which one between A and B is better than the other. Of course, it would be better to have enough data size, but it’s much better to be able to say, for example, “A is better than B with 60% probability” than “We don’t have enough data yet.” And you can decide if you want to wait longer or not at any time.

There are two things you need to know about Bayesian. One is the Prior and another is the Posterior. 

- The prior is basically the knowledge you had about the data before. For example, most likely you would know what would be your web site’s typical conversion rate before you even start the testing. You might say something like between 15 to 20%. In Bayesian, you can calculate the prior based on the average and standard deviation of the conversion rate for example.
- The posterior is the updated knowledge after the real data comes in.

Posterior = Data + Prior

There is an R package called `BayesAB` built and maintained by Frank Portman and it provides a simple and easy way to employ Bayesian inference methods for evaluating the A/B test results.

A prior lets you specify some sort of known information about a certain parameter. Priors can come from a variety of places including past experiments, literature, and domain expertise into the problem.

Priors can be weak or strong. A strong prior will convey a very precise belief as to where a parameter’s values may lie. For example:

```{r}
plotBeta(1000,1000)
```

Do you need (weak/strong) priors? Not necessarily. You can still leverage the interpretability benefits of Bayesian AB testing without priors. However, without priors of some kind you run into similar issues as with Frequentist AB testing, namely Type 1 and Type 2 errors. A Type 1 error is calling one version better when it really isn’t, and a Type 2 error is calling a better version equal or worse. Both typically arise from low sample size/base rate and are controlled by reaching appropriate sample size as per a power calculation.

How do you calculate priors?  It is simple - experiment with the tools provided my `BayesAB`.  Look at the code above.  Vary the values of the `plotBeta` function to simulate the current performace of the metric you are seeking to test.  As another example, suppose to have a conversion rate on a webapge you are considering to mdify.  The current converions rate is around 20% and a standard deviation of 4%.  Attempt to use different values (`alpha`, `beta`) in `plotBeta` to mimic your prior informaiton.

`aplha` tends to represent the mean and moves the curve left (low values) or right. `beta` tends to suggest the standard deviation and makes the curve fatter (low values) or slimmer.

```{r}
p1 <- plotBeta(20, 50) + ggtitle("alpha=20 & beta=50")

p2 <- plotBeta(30, 150) + ggtitle("alpha=30 & beta=150")

p3 <- plotBeta(80, 350) + ggtitle("alpha=80 & beta=350")

p4 <- plotBeta(90, 400) + ggtitle("alpha=90 & beta=400")

grid.arrange(p1, p2, p3, p4, ncol=2)

plotBeta(3,7)#just for fun

```

The last plot looks like a reasonable representation of out prior knowledge (current converions rate is around 20% and a standard deviation of 4%).



https://www.countbayesie.com/blog/2015/4/25/bayesian-ab-testing

http://www.machinegurning.com/rstats/bayes_r/

http://ucanalytics.com/blogs/bayesian-statistics-to-improve-ab-testing-digital-marketing-case-study-example-part-3/

